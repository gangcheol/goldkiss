{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title : \"08. CNN with Text\"\n",
    "author : \"CIC\"\n",
    "date : \"02/08/21\"\n",
    "categoris : [python]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPnLzlDqVHtN"
   },
   "source": [
    "# CNN with Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GHwR_xODVKJj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506, 1)\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 5ms/step - loss: 7995.8369\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3660.6780\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2083.4761\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1223.1989\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 682.9676\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 386.6265\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 257.0114\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 221.1516\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 204.3957\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 191.5367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1600e9cc590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# 데이터를 준비합니다.\n",
    "path = \"https://raw.githubusercontent.com/blackdew/ml-tensorflow/master/data/csv/boston.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "x_train = df[['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis',\n",
    "              'rad', 'tax','ptratio', 'b', 'lstat']]\n",
    "y_train = df[['medv']]\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "# 모델을 준비합니다.\n",
    "X = tf.keras.Input(shape=[13])\n",
    "H = tf.keras.layers.Dense(5, activation=\"relu\")(X)\n",
    "Y = tf.keras.layers.Dense(1)(H)\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss='mse')\n",
    "\n",
    "# 모델을 학습합니다.\n",
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_iYpBuJWLNa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 데이터를 준비합니다.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "# 모델을 만듭니다.\n",
    "X = tf.keras.Input(shape=[28, 28, 1])\n",
    "\n",
    "H = tf.keras.layers.Conv2D(32, 3, padding=\"same\")(X)\n",
    "H = tf.keras.layers.Activation('relu')(H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "H = tf.keras.layers.Dense(120, activation=\"relu\")(H)\n",
    "H = tf.keras.layers.Dense(84, activation=\"relu\")(H)\n",
    "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# 모델을 학습합니다.\n",
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkMmi8jhtoBS"
   },
   "source": [
    "# FinanceDataReader\n",
    "- https://github.com/financedata-org/FinanceDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JecoXtJgbJqg"
   },
   "outputs": [],
   "source": [
    "!pip install finance-datareader\n",
    "# bokeh 버전 조정.\n",
    "!pip uninstall bokeh -y\n",
    "!pip install bokeh==2.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ocVxglFvfKg"
   },
   "outputs": [],
   "source": [
    "import bokeh\n",
    "print(bokeh.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qde0rd8wlw1i"
   },
   "outputs": [],
   "source": [
    "import FinanceDataReader as fdr\n",
    "\n",
    "# KT 주식\n",
    "df = fdr.DataReader('030200', '2023-08-01')\n",
    "\n",
    "# 캔들차트 그리기\n",
    "fdr.chart.plot(df, title='KT(030200)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtdMwvHzdM4d"
   },
   "source": [
    "# 주가 예측 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Giryepd3dLai"
   },
   "outputs": [],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import numpy as np\n",
    "\n",
    "# 050201 전체 (1999-05-31 ~ 현재)\n",
    "df = fdr.DataReader('050201')\n",
    "data = df.values[:, :-1]\n",
    "print(data.shape)\n",
    "\n",
    "# train, val, test 분리\n",
    "train, val, test = data[:-900], data[-900:-600], data[-600:]\n",
    "print(train.shape, val.shape, test.shape)\n",
    "\n",
    "# normailze - 표준화\n",
    "norm = {'std': train.std(axis=0), 'mean': train.mean(axis=0)}\n",
    "train = (train - norm['std']) / norm['mean']\n",
    "val = (val - norm['std']) / norm['mean']\n",
    "test = (test - norm['std']) / norm['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5p1Vg9QPdrbn"
   },
   "outputs": [],
   "source": [
    "# 독립/종속 분리\n",
    "x_train = np.array([train[i:i+10] for i in range(len(train) - 10)])\n",
    "y_train = np.array([train[i+10, 3] for i in range(len(train) - 10)])\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "x_val = np.array([val[i:i+10] for i in range(len(val) - 10)])\n",
    "y_val = np.array([val[i+10, 3] for i in range(len(val) - 10)])\n",
    "print(x_val.shape, y_val.shape)\n",
    "\n",
    "x_test = np.array([test[i:i+10] for i in range(len(test) - 10)])\n",
    "y_test = np.array([test[i+10, 3] for i in range(len(test) - 10)])\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvYQrdZajE0H"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# vanilla RNN\n",
    "X = tf.keras.layers.Input(shape=(10, 5))\n",
    "H = tf.keras.layers.SimpleRNN(8)(X)\n",
    "Y = tf.keras.layers.Dense(1)(H)\n",
    "\n",
    "model = tf.keras.models.Model(X, Y)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EneHEA_ll1O"
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.fit(x_train, y_train, epochs=100,\n",
    "          batch_size=128, validation_data=(x_val, y_val))\n",
    "\n",
    "# 평가\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thknf8-K8zkr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_pred)\n",
    "plt.plot(y_test)\n",
    "plt.legend(['pred', 'real'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-DKECbSXSEn"
   },
   "source": [
    "# MNIST 이미지 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdzH3iirXWfR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "# (60000, 28, 28) (60000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bzBPDK9ZFei"
   },
   "source": [
    "## with SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ7Ws4rIXoP2"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "X = tf.keras.layers.Input(shape=[28, 28])\n",
    "H = tf.keras.layers.SimpleRNN(32)(X)\n",
    "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3v6CexgXj6e"
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.fit(x_train, y_train, epochs=10,\n",
    "          batch_size=128, validation_split=0.2)\n",
    "\n",
    "# 평가\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxLePHPUXySH"
   },
   "source": [
    "## with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTASr7fNXxpb"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "X = tf.keras.layers.Input(shape=[28, 28])\n",
    "H = tf.keras.layers.LSTM(32)(X)\n",
    "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veKpDnWFX2zr"
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.fit(x_train, y_train, epochs=10,\n",
    "          batch_size=128, validation_split=0.2)\n",
    "\n",
    "# 평가\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMOpKnDpd1QS"
   },
   "source": [
    "# 텍스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wg15aEHwsml"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train = tf.keras.utils.pad_sequences(x_train, maxlen=20)\n",
    "x_test = tf.keras.utils.pad_sequences(x_test, maxlen=20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5wGQJrKY7kB"
   },
   "source": [
    "## 리뷰 글 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loOP00YXd_y6"
   },
   "outputs": [],
   "source": [
    "word2index = tf.keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i, w) for w, i in word2index.items())\n",
    "print(sorted(list(index2word.items()))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JafblAAPeN3f"
   },
   "outputs": [],
   "source": [
    "decoded = \" \".join(index2word[i] for i in x_train[1])\n",
    "print(x_train[1][:10])\n",
    "print(decoded[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bwwuNq6ZNDb"
   },
   "source": [
    "## with SimpleRNN + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDqWIBu1lWSm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.keras.Input(shape=[20])\n",
    "H = tf.keras.layers.Embedding(88585, 28)(X)\n",
    "H = tf.keras.layers.SimpleRNN(32)(H)\n",
    "Y = tf.keras.layers.Dense(1, activation=\"sigmoid\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"binary_crossentropy\", metrics=\"accuracy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxONJKhdnmQM"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLRI1o-jOh6r"
   },
   "source": [
    "## with Hidden Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBAFf2igOqmJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.keras.Input(shape=[20])\n",
    "\n",
    "H = tf.keras.layers.Embedding(88585, 28)(X)\n",
    "H = tf.keras.layers.SimpleRNN(32, return_sequences=True)(H)\n",
    "H = tf.keras.layers.GlobalAveragePooling1D()(H)\n",
    "Y = tf.keras.layers.Dense(1, activation=\"sigmoid\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"binary_crossentropy\", metrics=\"accuracy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53K9fFkLOsuq"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4hUGH4iHE9c"
   },
   "source": [
    "## with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQFA49TAnrWj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.keras.Input(shape=[20])\n",
    "\n",
    "H = tf.keras.layers.Embedding(88585, 28)(X)\n",
    "H = tf.keras.layers.SimpleRNN(32, return_sequences=True)(H)\n",
    "\n",
    "# Transformer::self-attentions\n",
    "H1 = tf.keras.layers.MultiHeadAttention(2, 16)(H, H)\n",
    "H = tf.keras.layers.BatchNormalization()(H + H1)\n",
    "\n",
    "# Transformer::feed-forward\n",
    "H1 = tf.keras.layers.Dense(32, activation='swish')(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H + H1)\n",
    "\n",
    "H = tf.keras.layers.GlobalAveragePooling1D()(H)\n",
    "Y = tf.keras.layers.Dense(1, activation=\"sigmoid\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"binary_crossentropy\", metrics=\"accuracy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FvJO2qHJ1Nk"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjvXr6VeyXv6"
   },
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfml8QijhHnc"
   },
   "outputs": [],
   "source": [
    "# NLTK의 데이터 다운로드 (한 번만 수행하면 됨)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 샘플 텍스트 데이터\n",
    "text1 = \"\"\"Natural language processing (NLP) is a field\n",
    "of computer science, artificial intelligence,\n",
    "and computational linguistics concerned with\n",
    "the interactions between computers and human\n",
    "(natural) languages.\"\"\"\n",
    "\n",
    "text2 = \"\"\"자연어 처리(Natural Language Processing, NLP)는\n",
    "인간의 언어 현상을 컴퓨터와 같은 기계를 이용하여\n",
    "모사할 수 있도록 하는 인공 지능의 하위 분야 중 하나입니다.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLJdeJjop9jH"
   },
   "source": [
    "## 한글 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXvrnmgvp-_V"
   },
   "outputs": [],
   "source": [
    "!pip install konlpy\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01Qhhq5_p_dd"
   },
   "outputs": [],
   "source": [
    "!ls /usr/share/fonts/truetype/nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kpj1h6iiqCLc"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "\n",
    "# Colab 의 한글 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "\n",
    "# 유니코드에서  음수 부호설정\n",
    "mpl.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6wJUQUfpv9Q"
   },
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc0qQjnahTFB",
    "outputId": "b84acd5e-d7d3-4528-b716-390938de413e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', ',', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 말뭉치 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 텍스트 토큰화 (Tokenization)\n",
    "tokens1 = nltk.tokenize.word_tokenize(text1)\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDq7l5ZWimpe",
    "outputId": "22f62542-eff7-4524-bc1f-255f9d51b83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어', '처리', '(', 'Natural', 'Language', 'Processing', ',', 'NLP', ')', '는', '\\n', '인간', '의', '언어', '현상', '을', '컴퓨터', '와', '같은', '기계', '를', '이용', '하여', '\\n', '모사', '할', '수', '있도록', '하는', '인공', '지능', '의', '하위', '분야', '중', '하나', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# KoNLPy에서 Okt 형태소 분석기를 사용\n",
    "okt = Okt()\n",
    "\n",
    "# 텍스트 토큰화 (Tokenization)\n",
    "tokens2 = okt.morphs(text2)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DlGGvm5pydr"
   },
   "source": [
    "## 정제 & 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4inFZNqj4Eb"
   },
   "outputs": [],
   "source": [
    "# 정제 (Cleaning) 및 정규화 (Normalization)\n",
    "\n",
    "cleaned_tokens1 = [token.lower() for token in tokens1 if token.isalnum()]\n",
    "print(cleaned_tokens1)\n",
    "\n",
    "cleaned_tokens2 = [token for token in tokens2 if token.isalnum()]\n",
    "print(cleaned_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGv4lLQFq382"
   },
   "outputs": [],
   "source": [
    "# 어간 추출 (Stemming)\n",
    "stemmer1 = nltk.stem.PorterStemmer()\n",
    "stemmed_tokens1 = [stemmer1.stem(token) for token in cleaned_tokens1]\n",
    "print(stemmed_tokens1)\n",
    "\n",
    "# 표제어 추출 (Lemmatization)\n",
    "nltk.download('wordnet')\n",
    "lemmatizer1 = nltk.stem.WordNetLemmatizer()\n",
    "lemmatized_tokens1 = [lemmatizer1.lemmatize(token) for token in cleaned_tokens1]\n",
    "print(lemmatizer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEOFLL9ZtX5P"
   },
   "source": [
    "## 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtaScaBvh-KR"
   },
   "outputs": [],
   "source": [
    "# 불용어 (Stopword) 제거\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words1 = set(nltk.corpus.stopwords.words('english'))\n",
    "filtered_tokens1 = [token for token in lemmatized_tokens1 if token not in stop_words1]\n",
    "print(filtered_tokens1)\n",
    "\n",
    "stop_words2 = set([\"은\", \"는\", \"이\", \"가\", \"을\", \"를\"])\n",
    "filtered_tokens2 = [token for token in cleaned_tokens2 if token not in stop_words2]\n",
    "print(filtered_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KpjmACwaQhv"
   },
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_I6iJPHf7BV"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 워드클라우드 생성\n",
    "wordcloud1 = WordCloud(width=800, height=400, background_color='white')\n",
    "# 폰트 변경\n",
    "wordcloud1.font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "\n",
    "wordcloud1.generate(' '.join(filtered_tokens1))\n",
    "wordcloud1.to_file(\"wordcloud1.png\")\n",
    "\n",
    "# 워드클라우드 출력\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AzD-vhhvO5q"
   },
   "outputs": [],
   "source": [
    "# 워드클라우드 생성\n",
    "wordcloud2 = WordCloud(width=800, height=400, background_color='white')\n",
    "# 폰트 변경\n",
    "wordcloud2.font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "\n",
    "wordcloud2.generate(' '.join(filtered_tokens2))\n",
    "wordcloud2.to_file(\"wordcloud1.png\")\n",
    "\n",
    "# 워드클라우드 출력\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWE3a2S2yZsx"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 워드클라우드 출력\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUJ2_cjmyqgL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
