[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Goldkiss",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html",
    "href": "posts/basic/2023-11-01-03. Titanic.html",
    "title": "03. Titanic",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams[\"font.family\"] = \"Malgun Gothic\"\nplt.rcParams[\"axes.unicode_minus\"] = False\n\n\ndf = sns.load_dataset(\"titanic\")\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#pclass분포-확인",
    "href": "posts/basic/2023-11-01-03. Titanic.html#pclass분포-확인",
    "title": "03. Titanic",
    "section": "pclass분포 확인",
    "text": "pclass분포 확인\n\ndf.pclass.value_counts()\n\npclass\n3    491\n1    216\n2    184\nName: count, dtype: int64\n\n\n\ndf.pclass.value_counts().plot(kind = \"bar\")\n\n&lt;Axes: xlabel='pclass'&gt;"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#sex-값-분포-확인",
    "href": "posts/basic/2023-11-01-03. Titanic.html#sex-값-분포-확인",
    "title": "03. Titanic",
    "section": "sex 값 분포 확인",
    "text": "sex 값 분포 확인\n\ndf.sex.value_counts().plot(kind = \"bar\")\n\n&lt;Axes: xlabel='sex'&gt;"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#age-boxplot",
    "href": "posts/basic/2023-11-01-03. Titanic.html#age-boxplot",
    "title": "03. Titanic",
    "section": "age boxplot",
    "text": "age boxplot\n\ndf.age.plot(kind = \"box\")\n\n&lt;Axes: &gt;\n\n\n\n\n\n\npclass별 age 분포확인\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\ndf.plot(y = \"age\", kind = \"box\", backend = \"plotly\",\n        color = \"pclass\")\n\n\n                                                \n\n\n\n\npclass별 age 분포 + Survived 추가\n\ndf.plot(x = \"pclass\", y = \"age\", kind = \"box\", backend = \"plotly\",\n        color = \"survived\")"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#pclass별-생존률-평균-구하기",
    "href": "posts/basic/2023-11-01-03. Titanic.html#pclass별-생존률-평균-구하기",
    "title": "03. Titanic",
    "section": "pclass별 생존률 평균 구하기",
    "text": "pclass별 생존률 평균 구하기\n- 결과를 pclass_grp 변수에 저장\n\ntarget = \"survived\"\n\n\npclass_grp = df.groupby(\"pclass\", as_index = True)[[target]].mean().reset_index()\n\n\npclass_grp\n\n\n\n\n\n\n\n\npclass\nsurvived\n\n\n\n\n0\n1\n0.629630\n\n\n1\n2\n0.472826\n\n\n2\n3\n0.242363\n\n\n\n\n\n\n\n\npclass_grp.plot(kind = \"bar\", backend = \"plotly\",\n                x = \"pclass\", y = target,color = \"pclass\", width = 400, height = 400)"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#중복컬럼삭제",
    "href": "posts/basic/2023-11-01-03. Titanic.html#중복컬럼삭제",
    "title": "03. Titanic",
    "section": "(1) 중복컬럼삭제",
    "text": "(1) 중복컬럼삭제\n\ndf.drop([\"class\",\"alive\"], axis = 1, inplace = True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   who          891 non-null    object  \n 9   adult_male   891 non-null    bool    \n 10  deck         203 non-null    category\n 11  embark_town  889 non-null    object  \n 12  alone        891 non-null    bool    \ndtypes: bool(2), category(1), float64(2), int64(4), object(4)\nmemory usage: 72.7+ KB"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#필요없는-열-삭제",
    "href": "posts/basic/2023-11-01-03. Titanic.html#필요없는-열-삭제",
    "title": "03. Titanic",
    "section": "(2) 필요없는 열 삭제",
    "text": "(2) 필요없는 열 삭제\n\nd_col = ['embarked', 'who', 'adult_male', 'deck', 'embark_town', 'alone'] \n\ndf.drop(d_col, axis = 1, inplace = True)\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   survived  891 non-null    int64  \n 1   pclass    891 non-null    int64  \n 2   sex       891 non-null    object \n 3   age       714 non-null    float64\n 4   sibsp     891 non-null    int64  \n 5   parch     891 non-null    int64  \n 6   fare      891 non-null    float64\ndtypes: float64(2), int64(4), object(1)\nmemory usage: 48.9+ KB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#성별-컬럼-정수-인코딩",
    "href": "posts/basic/2023-11-01-03. Titanic.html#성별-컬럼-정수-인코딩",
    "title": "03. Titanic",
    "section": "(3) 성별 컬럼 정수 인코딩",
    "text": "(3) 성별 컬럼 정수 인코딩\n\ndf[\"sex\"].replace([\"male\",\"female\"], [0, 1], inplace = True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\n\n\n\n\n0\n0\n3\n0\n22.0\n1\n0\n7.2500\n\n\n1\n1\n1\n1\n38.0\n1\n0\n71.2833\n\n\n2\n1\n3\n1\n26.0\n0\n0\n7.9250\n\n\n3\n1\n1\n1\n35.0\n1\n0\n53.1000\n\n\n4\n0\n3\n0\n35.0\n0\n0\n8.0500"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#결측치-확인",
    "href": "posts/basic/2023-11-01-03. Titanic.html#결측치-확인",
    "title": "03. Titanic",
    "section": "(4) 결측치 확인",
    "text": "(4) 결측치 확인\n\ndf.isnull().sum()\n\nsurvived      0\npclass        0\nsex           0\nage         177\nsibsp         0\nparch         0\nfare          0\ndtype: int64"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#결측치-평균값으로-대체",
    "href": "posts/basic/2023-11-01-03. Titanic.html#결측치-평균값으로-대체",
    "title": "03. Titanic",
    "section": "(5) 결측치 평균값으로 대체",
    "text": "(5) 결측치 평균값으로 대체\n\nm = df.age.mean()\n\ndf.age.fillna(m, inplace = True)\n\n\ndf.isnull().sum()\n\nsurvived    0\npclass      0\nsex         0\nage         0\nsibsp       0\nparch       0\nfare        0\ndtype: int64"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#tree",
    "href": "posts/basic/2023-11-01-03. Titanic.html#tree",
    "title": "03. Titanic",
    "section": "(1) tree",
    "text": "(1) tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth = 5)\n\ntree.fit(X_train, y_train)\n\ntree_pred = tree.predict(X_test)\n\nfrom sklearn.metrics import *\n\naccuracy_score(y_test, tree_pred)\n\n0.8044692737430168"
  },
  {
    "objectID": "posts/basic/2023-11-01-03. Titanic.html#rf",
    "href": "posts/basic/2023-11-01-03. Titanic.html#rf",
    "title": "03. Titanic",
    "section": "(2) RF",
    "text": "(2) RF\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth=5, n_estimators = 10)\n\nrf.fit(X_train, y_train)\n\nrf_pred = rf.predict(X_test)\n\naccuracy_score(y_test, rf_pred)\n\n0.7932960893854749"
  },
  {
    "objectID": "posts/basic/2023-10-31-01. VOC.html",
    "href": "posts/basic/2023-10-31-01. VOC.html",
    "title": "01. VOC",
    "section": "",
    "text": "#!pip install tensorflow\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.dafault = \"plotly_mimetype+notebook_connected\"\nimport scipy.stats as spst\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\nplt.rcParams[\"font.family\"] = \"Malgun Gothic\"\nplt.rcParams[\"axes.unicode_minus\"] = False\n\n\n\nimport warnings\nwarnings.filterwarnings(action =\"ignore\")"
  },
  {
    "objectID": "posts/basic/2023-10-31-01. VOC.html#sklearn의-모듈을-읽어와-범주형-데이터를-카테고리화starstarstar",
    "href": "posts/basic/2023-10-31-01. VOC.html#sklearn의-모듈을-읽어와-범주형-데이터를-카테고리화starstarstar",
    "title": "01. VOC",
    "section": "sklearn의 모듈을 읽어와 범주형 데이터를 카테고리화(\\(\\star\\star\\star\\))",
    "text": "sklearn의 모듈을 읽어와 범주형 데이터를 카테고리화(\\(\\star\\star\\star\\))\n- cat_cols 데이터프레임에서 cust_clas_itg_cd 열의 범주형 데이터를 숫자로 인코딩하고, 그 결과를 le_cust_clas_itg_cd 열에 저장\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ncat_cols[\"le_cust_clas_itg_cd\"] = le.fit_transform(cat_cols[\"cust_clas_itg_cd\"])\n\n\ncat_cols.head()\n\n\n\n\n\n\n\n\ncust_clas_itg_cd\ncont_sttus_itg_cd\ncust_dtl_ctg_itg_cd\ntrm_yn\nle_cust_clas_itg_cd\n\n\n\n\n0\nF\n10001\n10003\nN\n0\n\n\n1\nG\n10001\n10002\nN\n1\n\n\n2\nG\n10001\n10003\nN\n1\n\n\n3\nL\n10001\n90024\nN\n5\n\n\n4\nG\n10001\n90024\nN\n1"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html",
    "title": "02. 이미지 분류",
    "section": "",
    "text": "Fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라.\n1 평가지표로 accuracy를 이용할 것\n2 epoch는 10으로 설정할 것\n3 optimizer는 adam을 이용할 것\n\n\n\nimport tensorflow as tf\n\nfrom keras.backend import *\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.datasets import *\nfrom keras.losses import *\nfrom keras.optimizers import *\n\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\n\nx_train.shape, y_train.shape\n\n((60000, 28, 28), (60000,))\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.imshow(x_test[1], cmap = \"Greys\")\n\n&lt;matplotlib.image.AxesImage at 0x7ca011e8f970&gt;\n\n\n\n\n\n\n\n\n- MinMax scaler 금지 \\(\\to\\) 태블러 데이터에 최적화되어있기 때문\n\n전처리 규칙은 train set을 따라야한다!\n\n\nM, m = x_train.max(), x_train.min()\n\ntrain_x  = ((x_train-m)/(M-m)).reshape(-1, 28, 28, 1)\ntest_x  = ((x_test-m)/(M-m)).reshape(-1, 28, 28, 1)\n\n\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n    \"x784\" -&gt; \"node1\"\n\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node20\"\n    \"x2\" -&gt; \"node20\"\n    \"..\" -&gt; \"node20\"\n    \"x784\" -&gt; \"node20\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"node1 \"\n    \"node2\" -&gt; \"node1 \"\n    \"...\" -&gt; \"node1 \"\n    \"node20\" -&gt; \"node1 \"\n\n    \"node1\" -&gt; \"node2 \"\n    \"node2\" -&gt; \"node2 \"\n    \"...\" -&gt; \"node2 \"\n    \"node20\" -&gt; \"node2 \"\n\n    \"node1\" -&gt; \"... \"\n    \"node2\" -&gt; \"... \"\n    \"...\" -&gt; \"... \"\n    \"node20\" -&gt; \"... \"\n\n    \"node1\" -&gt; \"node30 \"\n    \"node2\" -&gt; \"node30 \"\n    \"...\" -&gt; \"node30 \"\n    \"node20\" -&gt; \"node30 \"\n\n\n    label = \"Layer 2: relu\"\n}\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1 \" -&gt; \"y10\"\n    \"node2 \" -&gt; \"y10\"\n    \"... \" -&gt; \"y10\"\n    \"node30 \" -&gt; \"y10\"\n\n    \"node1 \" -&gt; \"y1\"\n    \"node2 \" -&gt; \"y1\"\n    \"... \" -&gt; \"y1\"\n    \"node30 \" -&gt; \"y1\"\n\n    \"node1 \" -&gt; \".\"\n    \"node2 \" -&gt; \".\"\n    \"... \" -&gt; \".\"\n    \"node30 \" -&gt; \".\"\n\n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n\n28*28\n\n784\n\n\n\nnp.unique(y_train)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n\n\n\nmodel.compile?\n\nObject `model.compile` not found.\n\n\n\nmodel = Sequential()\n\nmodel.add(Flatten())\nmodel.add(Dense(20, activation = \"relu\"))\nmodel.add(Dense(30, activation = \"relu\"))\nmodel.add(Dense(10, activation = \"softmax\"))\n\nmodel.compile(optimizer = Adam(0.001), metrics = [\"acc\"], loss =  \"sparse_categorical_crossentropy\")\n\n\nes  = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5, verbose = 1)\n\nmc = ModelCheckpoint(monitor = \"val_loss\", filepath = \"mnist_dnn.ckpt\",\n                                    save_best_only = True,  mode = \"min\", verbose = 1 )\n\n\n\n\n\nhistory = model.fit(train_x, y_train, epochs = 10, validation_split = 0.2,\n                                  callbacks = [es, mc]).history\n\nEpoch 1/10\n1485/1500 [============================&gt;.] - ETA: 0s - loss: 0.3112 - acc: 0.8847\nEpoch 1: val_loss did not improve from 0.35468\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.3110 - acc: 0.8849 - val_loss: 0.3716 - val_acc: 0.8689\nEpoch 2/10\n1489/1500 [============================&gt;.] - ETA: 0s - loss: 0.3018 - acc: 0.8887\nEpoch 2: val_loss did not improve from 0.35468\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.3020 - acc: 0.8888 - val_loss: 0.3825 - val_acc: 0.8664\nEpoch 3/10\n1483/1500 [============================&gt;.] - ETA: 0s - loss: 0.2999 - acc: 0.8894\nEpoch 3: val_loss did not improve from 0.35468\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.2999 - acc: 0.8895 - val_loss: 0.3568 - val_acc: 0.8772\nEpoch 4/10\n1499/1500 [============================&gt;.] - ETA: 0s - loss: 0.2959 - acc: 0.8922\nEpoch 4: val_loss did not improve from 0.35468\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.2958 - acc: 0.8923 - val_loss: 0.3665 - val_acc: 0.8738\nEpoch 5/10\n1490/1500 [============================&gt;.] - ETA: 0s - loss: 0.2912 - acc: 0.8904\nEpoch 5: val_loss did not improve from 0.35468\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.2911 - acc: 0.8905 - val_loss: 0.3589 - val_acc: 0.8723\nEpoch 6/10\n1499/1500 [============================&gt;.] - ETA: 0s - loss: 0.2865 - acc: 0.8951\nEpoch 6: val_loss improved from 0.35468 to 0.35428, saving model to mnist_dnn.ckpt\n1500/1500 [==============================] - 8s 5ms/step - loss: 0.2865 - acc: 0.8951 - val_loss: 0.3543 - val_acc: 0.8740\nEpoch 7/10\n1498/1500 [============================&gt;.] - ETA: 0s - loss: 0.2824 - acc: 0.8946\nEpoch 7: val_loss did not improve from 0.35428\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.2823 - acc: 0.8946 - val_loss: 0.3610 - val_acc: 0.8751\nEpoch 8/10\n1482/1500 [============================&gt;.] - ETA: 0s - loss: 0.2809 - acc: 0.8966\nEpoch 8: val_loss improved from 0.35428 to 0.35115, saving model to mnist_dnn.ckpt\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.2807 - acc: 0.8966 - val_loss: 0.3511 - val_acc: 0.8751\nEpoch 9/10\n1485/1500 [============================&gt;.] - ETA: 0s - loss: 0.2759 - acc: 0.8977\nEpoch 9: val_loss did not improve from 0.35115\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.2760 - acc: 0.8977 - val_loss: 0.3617 - val_acc: 0.8750\nEpoch 10/10\n1495/1500 [============================&gt;.] - ETA: 0s - loss: 0.2708 - acc: 0.9002\nEpoch 10: val_loss did not improve from 0.35115\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.2712 - acc: 0.9001 - val_loss: 0.3593 - val_acc: 0.8748\n\n\n\n\n\n\nresult1 = model.predict(test_x)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nresult1 = np.argmax(result1, axis = 1)\n\n\nfrom sklearn.metrics import *\n\naccuracy_score(y_test, result1)\n\n0.8652\n\n\n\n\n\n1 train_loss, val_loss 확인\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (8,4))\nplt.plot(history[\"loss\"], label = \"train_loss\")\nplt.plot(history[\"val_loss\"], label = \"val_loss\")\nplt.legend()\nplt.show()\n\n\n\n\n2 실젝값, 예측값 비교\n\nfrom sklearn.metrics import *\nimport seaborn as sns\n\nsns.heatmap(confusion_matrix(y_test, result1),\n                        annot = True, fmt = \"3d\")\n\nprint(classification_report(y_test, result1))\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.83      0.81      1000\n           1       0.96      0.97      0.97      1000\n           2       0.81      0.73      0.76      1000\n           3       0.87      0.86      0.87      1000\n           4       0.75      0.82      0.78      1000\n           5       0.95      0.96      0.96      1000\n           6       0.69      0.62      0.65      1000\n           7       0.93      0.95      0.94      1000\n           8       0.91      0.97      0.94      1000\n           9       0.97      0.94      0.95      1000\n\n    accuracy                           0.87     10000\n   macro avg       0.86      0.87      0.86     10000\nweighted avg       0.86      0.87      0.86     10000"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#적합된-네트워크를-이용하여-test-data의-accuracy를-구하라",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#적합된-네트워크를-이용하여-test-data의-accuracy를-구하라",
    "title": "02. 이미지 분류",
    "section": "",
    "text": "result1 = model.predict(test_x)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nresult1 = np.argmax(result1, axis = 1)\n\n\nfrom sklearn.metrics import *\n\naccuracy_score(y_test, result1)\n\n0.8652"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#과적합인지-확인",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#과적합인지-확인",
    "title": "02. 이미지 분류",
    "section": "",
    "text": "1 train_loss, val_loss 확인\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (8,4))\nplt.plot(history[\"loss\"], label = \"train_loss\")\nplt.plot(history[\"val_loss\"], label = \"val_loss\")\nplt.legend()\nplt.show()\n\n\n\n\n2 실젝값, 예측값 비교\n\nfrom sklearn.metrics import *\nimport seaborn as sns\n\nsns.heatmap(confusion_matrix(y_test, result1),\n                        annot = True, fmt = \"3d\")\n\nprint(classification_report(y_test, result1))\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.83      0.81      1000\n           1       0.96      0.97      0.97      1000\n           2       0.81      0.73      0.76      1000\n           3       0.87      0.86      0.87      1000\n           4       0.75      0.82      0.78      1000\n           5       0.95      0.96      0.96      1000\n           6       0.69      0.62      0.65      1000\n           7       0.93      0.95      0.94      1000\n           8       0.91      0.97      0.94      1000\n           9       0.97      0.94      0.95      1000\n\n    accuracy                           0.87     10000\n   macro avg       0.86      0.87      0.86     10000\nweighted avg       0.86      0.87      0.86     10000"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#데이터-로드-및-전처리",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#데이터-로드-및-전처리",
    "title": "02. 이미지 분류",
    "section": "데이터 로드 및 전처리",
    "text": "데이터 로드 및 전처리\n\n(x_train, y_train), (x_test, y_test)  = fashion_mnist.load_data()\n\n\nM, m = x_train.max(), x_train.min()\n\ntrain_x = ((x_train-m)/(M-m)).reshape(-1, 28, 28, 1)\ntest_x = ((x_test-m)/(M-m)).reshape(-1, 28, 28, 1)"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-설계-1",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-설계-1",
    "title": "02. 이미지 분류",
    "section": "모델 설계 1",
    "text": "모델 설계 1\n이때 n1=6, n2=16, n3=120 으로 설정한다, 드랍아웃비율은 20%로 설정한다.\n\nmodel  = Sequential()\n\nmodel.add(Conv2D(filters = 6, kernel_size = (5 ,5), input_shape = (28, 28, 1), padding = \"valid\")) ## 패딩은 \"valid\"가 기본값\nmodel.add(MaxPool2D(pool_size = (2,2),  padding = \"valid\")) ## 패딩과 pool_size는 입력한 값이 디폴트임\nmodel.add(Conv2D(16, (5, 5)))\nmodel.add(MaxPool2D())\nmodel.add(Flatten())\nmodel.add(Dense(120, activation = \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = \"softmax\"))\n\nmodel.compile(optimizer = Adam(0.001), loss = \"sparse_categorical_crossentropy\", metrics = [\"acc\"])\n\n\nmodel.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_8 (Conv2D)           (None, 24, 24, 6)         156       \n                                                                 \n max_pooling2d_7 (MaxPoolin  (None, 12, 12, 6)         0         \n g2D)                                                            \n                                                                 \n conv2d_9 (Conv2D)           (None, 8, 8, 16)          2416      \n                                                                 \n max_pooling2d_8 (MaxPoolin  (None, 4, 4, 16)          0         \n g2D)                                                            \n                                                                 \n flatten_6 (Flatten)         (None, 256)               0         \n                                                                 \n dense_15 (Dense)            (None, 120)               30840     \n                                                                 \n dropout_3 (Dropout)         (None, 120)               0         \n                                                                 \n dense_16 (Dense)            (None, 10)                1210      \n                                                                 \n=================================================================\nTotal params: 34622 (135.24 KB)\nTrainable params: 34622 (135.24 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-설계-2",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-설계-2",
    "title": "02. 이미지 분류",
    "section": "모델 설계 2",
    "text": "모델 설계 2\nn1=(6,64,128), n2=(16,256)에 대하여 test set의 loss가 최소화되는 조합을 찾아라.\n\nepoc = 3, validation = 0.2로 설정\n\n\nn1 = [6, 64, 128]\nn2 = [16, 256]\n\nfor i in n1 :\n  for j in n2 :\n      model  = Sequential()\n      model.add(Conv2D(filters = i, kernel_size = (5 ,5), input_shape = (28, 28, 1), padding = \"valid\")) ## 패딩은 \"valid\"가 기본값\n      model.add(MaxPool2D(pool_size = (2,2),  padding = \"valid\")) ## 패딩과 pool_size는 입력한 값이 디폴트임\n      model.add(Conv2D(j, (5, 5)))\n      model.add(MaxPool2D())\n      model.add(Flatten())\n      model.add(Dense(120, activation = \"relu\"))\n      model.add(Dropout(0.2))\n      model.add(Dense(10, activation = \"softmax\"))\n\n      model.compile(optimizer = Adam(0.001), loss = \"sparse_categorical_crossentropy\", metrics = [\"acc\"])\n      model.fit(train_x, y_train, validation_split = 0.2, epochs = 3, verbose = 0)\n\n      result = model.evaluate(test_x, y_test)\n      print(f\"n1 : {i}, n2 : {j} -&gt; loss : {result[0]}, acc  :  {result[1]}\")\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.3501 - acc: 0.8754\nn1 : 6, n2 : 16 -&gt; loss : 0.3500571548938751, acc  :  0.8754000067710876\n313/313 [==============================] - 1s 2ms/step - loss: 0.3233 - acc: 0.8802\nn1 : 6, n2 : 256 -&gt; loss : 0.32328951358795166, acc  :  0.8802000284194946\n313/313 [==============================] - 1s 3ms/step - loss: 0.3424 - acc: 0.8788\nn1 : 64, n2 : 16 -&gt; loss : 0.34240153431892395, acc  :  0.8787999749183655\n313/313 [==============================] - 1s 3ms/step - loss: 0.3065 - acc: 0.8891\nn1 : 64, n2 : 256 -&gt; loss : 0.30652520060539246, acc  :  0.8891000151634216\n313/313 [==============================] - 1s 3ms/step - loss: 0.3387 - acc: 0.8804\nn1 : 128, n2 : 16 -&gt; loss : 0.3387366831302643, acc  :  0.8804000020027161\n313/313 [==============================] - 1s 4ms/step - loss: 0.3110 - acc: 0.8912\nn1 : 128, n2 : 256 -&gt; loss : 0.31100359559059143, acc  :  0.8912000060081482\n\n\n- 64, 256 일 때 test_loss가 가장 작음"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#데이터-로드-및-전처리-1",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#데이터-로드-및-전처리-1",
    "title": "02. 이미지 분류",
    "section": "데이터 로드 및 전처리",
    "text": "데이터 로드 및 전처리\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n\nx_train.shape, y_train.shape, x_test. shape,y_test.shape\n\n((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 1))\n\n\n\ntrain_x = x_train/255.0\ntest_x = x_test/255.0\n\ny_train = y_train.reshape(-1,)\ny_test = y_test.reshape(-1,)\n\n\ny_test.shape\n\n(10000,)"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-설계-3",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-설계-3",
    "title": "02. 이미지 분류",
    "section": "모델 설계",
    "text": "모델 설계\n\nmodel  = Sequential()\n\nmodel.add(Conv2D(32, (3,3), input_shape = (32,32, 3)))\nmodel.add(Conv2D(32, (3,3)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2, 2), strides = (2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(64, (3,3)))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(MaxPool2D((2, 2), strides = (2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation = \"softmax\"))\n\nmodel.compile(optimizer = Adam(0.001), loss = \"sparse_categorical_crossentropy\", metrics = [\"acc\"])\n\n\nes = EarlyStopping(monitor = \"val_loss\", mode = \"min\", verbose = 1, patience = 5)\nch = ModelCheckpoint(filepath = \"CIFAR.h5\", monitor = \"val_loss\", verbose = 1,  save_best_only= True )\n\n\nmodel.fit(train_x, y_train, epochs = 30, validation_split = 0.2, callbacks = [es, ch])\n\nEpoch 1/30\n1243/1250 [============================&gt;.] - ETA: 0s - loss: 1.6670 - acc: 0.4495\nEpoch 1: val_loss improved from inf to 2.01761, saving model to CIFAR.h5\n1250/1250 [==============================] - 13s 7ms/step - loss: 1.6649 - acc: 0.4503 - val_loss: 2.0176 - val_acc: 0.3802\nEpoch 2/30\n  10/1250 [..............................] - ETA: 7s - loss: 1.4683 - acc: 0.4750 1250/1250 [==============================] - ETA: 0s - loss: 1.2204 - acc: 0.5806\nEpoch 2: val_loss improved from 2.01761 to 1.30872, saving model to CIFAR.h5\n1250/1250 [==============================] - 8s 6ms/step - loss: 1.2204 - acc: 0.5806 - val_loss: 1.3087 - val_acc: 0.5842\nEpoch 3/30\n1243/1250 [============================&gt;.] - ETA: 0s - loss: 1.0502 - acc: 0.6359\nEpoch 3: val_loss improved from 1.30872 to 1.11962, saving model to CIFAR.h5\n1250/1250 [==============================] - 8s 7ms/step - loss: 1.0508 - acc: 0.6355 - val_loss: 1.1196 - val_acc: 0.6230\nEpoch 4/30\n1245/1250 [============================&gt;.] - ETA: 0s - loss: 0.9571 - acc: 0.6714\nEpoch 4: val_loss improved from 1.11962 to 0.93383, saving model to CIFAR.h5\n1250/1250 [==============================] - 8s 7ms/step - loss: 0.9571 - acc: 0.6715 - val_loss: 0.9338 - val_acc: 0.6749\nEpoch 5/30\n1250/1250 [==============================] - ETA: 0s - loss: 0.8859 - acc: 0.6924\nEpoch 5: val_loss improved from 0.93383 to 0.86031, saving model to CIFAR.h5\n1250/1250 [==============================] - 8s 6ms/step - loss: 0.8859 - acc: 0.6924 - val_loss: 0.8603 - val_acc: 0.7047\nEpoch 6/30\n1243/1250 [============================&gt;.] - ETA: 0s - loss: 0.8145 - acc: 0.7167\nEpoch 6: val_loss did not improve from 0.86031\n1250/1250 [==============================] - 9s 7ms/step - loss: 0.8142 - acc: 0.7168 - val_loss: 0.9080 - val_acc: 0.6976\nEpoch 7/30\n1249/1250 [============================&gt;.] - ETA: 0s - loss: 0.7646 - acc: 0.7375\nEpoch 7: val_loss improved from 0.86031 to 0.84302, saving model to CIFAR.h5\n1250/1250 [==============================] - 8s 6ms/step - loss: 0.7648 - acc: 0.7375 - val_loss: 0.8430 - val_acc: 0.7168\nEpoch 8/30\n1243/1250 [============================&gt;.] - ETA: 0s - loss: 0.7157 - acc: 0.7530\nEpoch 8: val_loss did not improve from 0.84302\n1250/1250 [==============================] - 8s 6ms/step - loss: 0.7159 - acc: 0.7528 - val_loss: 0.8542 - val_acc: 0.7174\nEpoch 9/30\n1243/1250 [============================&gt;.] - ETA: 0s - loss: 0.6680 - acc: 0.7698\nEpoch 9: val_loss did not improve from 0.84302\n1250/1250 [==============================] - 10s 8ms/step - loss: 0.6687 - acc: 0.7694 - val_loss: 0.8723 - val_acc: 0.7212\nEpoch 10/30\n1245/1250 [============================&gt;.] - ETA: 0s - loss: 0.6272 - acc: 0.7805\nEpoch 10: val_loss improved from 0.84302 to 0.78872, saving model to CIFAR.h5\n1250/1250 [==============================] - 9s 7ms/step - loss: 0.6279 - acc: 0.7804 - val_loss: 0.7887 - val_acc: 0.7422\nEpoch 11/30\n1248/1250 [============================&gt;.] - ETA: 0s - loss: 0.5885 - acc: 0.7975\nEpoch 11: val_loss did not improve from 0.78872\n1250/1250 [==============================] - 8s 6ms/step - loss: 0.5884 - acc: 0.7974 - val_loss: 0.8317 - val_acc: 0.7329\nEpoch 12/30\n1242/1250 [============================&gt;.] - ETA: 0s - loss: 0.5561 - acc: 0.8071\nEpoch 12: val_loss did not improve from 0.78872\n1250/1250 [==============================] - 9s 7ms/step - loss: 0.5563 - acc: 0.8072 - val_loss: 0.8331 - val_acc: 0.7350\nEpoch 13/30\n1245/1250 [============================&gt;.] - ETA: 0s - loss: 0.5360 - acc: 0.8161\nEpoch 13: val_loss did not improve from 0.78872\n1250/1250 [==============================] - 8s 7ms/step - loss: 0.5360 - acc: 0.8161 - val_loss: 0.8569 - val_acc: 0.7379\nEpoch 14/30\n1246/1250 [============================&gt;.] - ETA: 0s - loss: 0.5022 - acc: 0.8245\nEpoch 14: val_loss did not improve from 0.78872\n1250/1250 [==============================] - 8s 6ms/step - loss: 0.5022 - acc: 0.8244 - val_loss: 0.8388 - val_acc: 0.7460\nEpoch 15/30\n1244/1250 [============================&gt;.] - ETA: 0s - loss: 0.4688 - acc: 0.8346\nEpoch 15: val_loss did not improve from 0.78872\n1250/1250 [==============================] - 9s 7ms/step - loss: 0.4691 - acc: 0.8344 - val_loss: 0.8866 - val_acc: 0.7282\nEpoch 15: early stopping\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n\n\n&lt;keras.src.callbacks.History at 0x79059019eb00&gt;"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#예측",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#예측",
    "title": "02. 이미지 분류",
    "section": "예측",
    "text": "예측\n\nmodel.evaluate(test_x, y_test)\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.9003 - acc: 0.7314\n\n\n[0.9002749919891357, 0.7314000129699707]"
  },
  {
    "objectID": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-로드-후-예측-결과-시각화",
    "href": "posts/advanced/2024-01-22-02.이미지_분류.html#모델-로드-후-예측-결과-시각화",
    "title": "02. 이미지 분류",
    "section": "모델 로드 후 예측 결과 시각화",
    "text": "모델 로드 후 예측 결과 시각화\n\nload_model = load_model(\"CIFAR.h5\")\n\n\nimport numpy as np\npred = np.argmax(load_model.predict(test_x), axis = 1)\n\n313/313 [==============================] - 2s 5ms/step\n\n\n\nimport seaborn as sns\nfrom sklearn.metrics import *\n\nsns.heatmap(confusion_matrix(y_test, pred),\n                                                      annot = True, fmt = \"3d\")\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/advanced/2024-01-21-00. Associate review.html",
    "href": "posts/advanced/2024-01-21-00. Associate review.html",
    "title": "00. Associate Review",
    "section": "",
    "text": "1 반드시 답안을 작성하라는 셀안에 코드를 작성하기\n2 변수명이 제시된 경우 반드시 해당 변수명을 사용하기\n3 데이터를 분석 및 전처리한 후 머신러닝과 딥러닝으로 VOC를 제기한 고객의 해지여부를 예측하고 결과를 분석하세요."
  },
  {
    "objectID": "posts/advanced/2024-01-21-00. Associate review.html#voc_trt_perd_itg_cd-변수의-고유값-count",
    "href": "posts/advanced/2024-01-21-00. Associate review.html#voc_trt_perd_itg_cd-변수의-고유값-count",
    "title": "00. Associate Review",
    "section": "voc_trt_perd_itg_cd 변수의 고유값 count",
    "text": "voc_trt_perd_itg_cd 변수의 고유값 count\n\ndf[\"voc_trt_perd_itg_cd\"].value_counts()\n\n_        5422\n10000    4283\n10001     163\n10002      58\n10003      25\n10004      16\n10005      10\n10006       6\n10008       3\n10009       3\n10016       2\n10011       2\n10012       2\n10007       2\n10014       1\n10013       1\n10015       1\nName: voc_trt_perd_itg_cd, dtype: int64\n\n\n\ndf[\"voc_trt_perd_itg_cd\"].value_counts(normalize = True)\n\n_        0.5422\n10000    0.4283\n10001    0.0163\n10002    0.0058\n10003    0.0025\n10004    0.0016\n10005    0.0010\n10006    0.0006\n10008    0.0003\n10009    0.0003\n10016    0.0002\n10011    0.0002\n10012    0.0002\n10007    0.0002\n10014    0.0001\n10013    0.0001\n10015    0.0001\nName: voc_trt_perd_itg_cd, dtype: float64"
  },
  {
    "objectID": "posts/advanced/2024-01-21-00. Associate review.html#라벨인코딩",
    "href": "posts/advanced/2024-01-21-00. Associate review.html#라벨인코딩",
    "title": "00. Associate Review",
    "section": "라벨인코딩",
    "text": "라벨인코딩\ncat_cols 데이터프레임에서 cust_clas_itg_cd 열의 범주형 데이터를 숫자로 인코딩하고, 그 결과를 le_cust_clas_itg_cd 열에 저장\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ncat_cols[\"le_cust_clas_itg_cd\"] = le.fit_transform(cat_cols[\"cust_clas_itg_cd\"])\n\n\ncat_cols.head()\n\n\n  \n    \n\n\n\n\n\n\ncust_clas_itg_cd\ncont_sttus_itg_cd\ncust_dtl_ctg_itg_cd\ntrm_yn\nle_cust_clas_itg_cd\n\n\n\n\n0\nF\n10001\n10003\nN\n0\n\n\n1\nG\n10001\n10002\nN\n1\n\n\n2\nG\n10001\n10003\nN\n1\n\n\n3\nL\n10001\n90024\nN\n5\n\n\n4\nG\n10001\n90024\nN\n1"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html",
    "href": "posts/advanced/2024-01-08-04. MNIST.html",
    "title": "04. MNIST",
    "section": "",
    "text": "import tensorflow as tf\nfrom tensorflow import keras"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#one-hot-encoding",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#one-hot-encoding",
    "title": "04. MNIST",
    "section": "one-hot encoding",
    "text": "one-hot encoding\n\nfrom keras.utils import to_categorical\n\n\ntrain_y_c = to_categorical(y_train, 10)\ntest_y_c = to_categorical(y_test, 10)"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#import-1",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#import-1",
    "title": "04. MNIST",
    "section": "import",
    "text": "import\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#모델-설계",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#모델-설계",
    "title": "04. MNIST",
    "section": "모델 설계",
    "text": "모델 설계\n\n# 1. 세션 클리어\nclear_session()\n\n# 2. 모델 설계\n\nmodel1 = Sequential()\n\nmodel1.add( Input(shape = (28,28,1)))\nmodel1.add( Flatten() )\nmodel1.add( Dense(1024, activation = \"relu\"))\nmodel1.add( Dense(1024, activation = \"relu\"))\nmodel1.add( BatchNormalization())\nmodel1.add( Dropout(0.25) )\n\nmodel1.add( Dense(512, activation = \"relu\"))\nmodel1.add( Dense(512, activation = \"relu\"))\nmodel1.add( BatchNormalization())\nmodel1.add( Dropout(0.25) )\n\nmodel1.add( Dense (10, activation = \"softmax\"))\n\n# 3. 모델 컴파일\nmodel1.compile(optimizer = \"adam\", loss = tf.keras.losses.categorical_crossentropy,\n               metrics = [\"accuracy\"])\n\n\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 1024)              803840    \n                                                                 \n dense_1 (Dense)             (None, 1024)              1049600   \n                                                                 \n batch_normalization (Batch  (None, 1024)              4096      \n Normalization)                                                  \n                                                                 \n dropout (Dropout)           (None, 1024)              0         \n                                                                 \n dense_2 (Dense)             (None, 512)               524800    \n                                                                 \n dense_3 (Dense)             (None, 512)               262656    \n                                                                 \n batch_normalization_1 (Bat  (None, 512)               2048      \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 512)               0         \n                                                                 \n dense_4 (Dense)             (None, 10)                5130      \n                                                                 \n=================================================================\nTotal params: 2652170 (10.12 MB)\nTrainable params: 2649098 (10.11 MB)\nNon-trainable params: 3072 (12.00 KB)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#학습",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#학습",
    "title": "04. MNIST",
    "section": "학습",
    "text": "학습\n\nfrom keras.callbacks import EarlyStopping\n\n\nes = EarlyStopping(monitor = \"val_loss\",\n                  min_delta = 0,\n                  patience = 3,\n                   verbose = 1,\n                   restore_best_weights = True)\n\n\nhistory = model1.fit(train_x_rel, train_y_c,\n                     epochs = 10000, verbose = 1,\n                     validation_split = 0.2,\n                     callbacks = [es]).history\n\nEpoch 1/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.5921 - accuracy: 0.7939 - val_loss: 0.5014 - val_accuracy: 0.8234\nEpoch 2/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.4375 - accuracy: 0.8434 - val_loss: 0.5076 - val_accuracy: 0.8056\nEpoch 3/10000\n1500/1500 [==============================] - 14s 9ms/step - loss: 0.3908 - accuracy: 0.8593 - val_loss: 0.4336 - val_accuracy: 0.8573\nEpoch 4/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.3534 - accuracy: 0.8709 - val_loss: 0.3595 - val_accuracy: 0.8655\nEpoch 5/10000\n1500/1500 [==============================] - 14s 10ms/step - loss: 0.3349 - accuracy: 0.8790 - val_loss: 0.3693 - val_accuracy: 0.8729\nEpoch 6/10000\n1500/1500 [==============================] - 13s 9ms/step - loss: 0.3183 - accuracy: 0.8831 - val_loss: 0.3335 - val_accuracy: 0.8810\nEpoch 7/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.3068 - accuracy: 0.8870 - val_loss: 0.3397 - val_accuracy: 0.8715\nEpoch 8/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2890 - accuracy: 0.8942 - val_loss: 0.3358 - val_accuracy: 0.8773\nEpoch 9/10000\n1500/1500 [==============================] - 12s 8ms/step - loss: 0.2774 - accuracy: 0.8978 - val_loss: 0.3327 - val_accuracy: 0.8827\nEpoch 10/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2664 - accuracy: 0.9011 - val_loss: 0.3199 - val_accuracy: 0.8850\nEpoch 11/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.2569 - accuracy: 0.9037 - val_loss: 0.3585 - val_accuracy: 0.8722\nEpoch 12/10000\n1500/1500 [==============================] - 15s 10ms/step - loss: 0.2463 - accuracy: 0.9079 - val_loss: 0.3201 - val_accuracy: 0.8879\nEpoch 13/10000\n1498/1500 [============================&gt;.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9102Restoring model weights from the end of the best epoch: 10.\n1500/1500 [==============================] - 10s 6ms/step - loss: 0.2413 - accuracy: 0.9100 - val_loss: 0.3282 - val_accuracy: 0.8843\nEpoch 13: early stopping"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#결과-시각화",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#결과-시각화",
    "title": "04. MNIST",
    "section": "결과 시각화",
    "text": "결과 시각화\n\nplt.figure(figsize = (12,4))\nplt.plot(history[\"loss\"],\"--.\",label = \"train_loss\")\nplt.plot(history[\"val_loss\"], \"--.\",label = \"val_loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#예측",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#예측",
    "title": "04. MNIST",
    "section": "예측",
    "text": "예측\n\ny_pred =  model1.predict(test_x_rel).argmax(axis = 1)\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\nfrom sklearn.metrics import *\n\n\nconfusion_matrix(y_test, y_pred)\n\narray([[875,   0,  18,  27,   1,   0,  70,   0,   9,   0],\n       [  5, 958,   3,  30,   1,   0,   2,   0,   1,   0],\n       [ 19,   0, 842,  15,  65,   0,  55,   0,   4,   0],\n       [ 26,   2,   9, 920,  17,   0,  22,   0,   4,   0],\n       [  1,   0, 146,  52, 721,   0,  73,   0,   7,   0],\n       [  0,   0,   0,   0,   0, 987,   0,   9,   2,   2],\n       [162,   0, 109,  31,  48,   0, 639,   0,  11,   0],\n       [  0,   0,   0,   0,   0,  52,   0, 921,   0,  27],\n       [  5,   0,  10,   2,   5,   4,   0,   4, 970,   0],\n       [  0,   0,   0,   0,   0,  17,   1,  27,   0, 955]])\n\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.88      0.84      1000\n           1       1.00      0.96      0.98      1000\n           2       0.74      0.84      0.79      1000\n           3       0.85      0.92      0.89      1000\n           4       0.84      0.72      0.78      1000\n           5       0.93      0.99      0.96      1000\n           6       0.74      0.64      0.69      1000\n           7       0.96      0.92      0.94      1000\n           8       0.96      0.97      0.97      1000\n           9       0.97      0.95      0.96      1000\n\n    accuracy                           0.88     10000\n   macro avg       0.88      0.88      0.88     10000\nweighted avg       0.88      0.88      0.88     10000"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#import-2",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#import-2",
    "title": "04. MNIST",
    "section": "import",
    "text": "import\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.backend import clear_session\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input, Conv2D, MaxPool2D"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#모델-설계-1",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#모델-설계-1",
    "title": "04. MNIST",
    "section": "모델 설계",
    "text": "모델 설계\n\n\n# 1. 세션클리어\nkeras.backend.clear_session()\n\n# 2. 모델 설계\nmodel2 = Sequential()\n\nmodel2.add(Input(shape = (28, 28, 1)))\n\nmodel2.add( Conv2D(filters = 28, kernel_size = (3,3),\n            strides = (1,1), padding = \"same\",\n             activation = \"relu\"))\n\nmodel2.add( Conv2D(filters = 28, kernel_size = (3,3),\n            strides = (1,1), padding = \"same\",\n             activation = \"relu\"))\n\nmodel2.add( BatchNormalization() )\nmodel2.add(MaxPool2D(pool_size = (2,2), strides= (2,2)))\nmodel2.add ( keras.layers.Dropout(0.25) )\n\nmodel2.add( keras.layers.Flatten() )\n# Fully Connected Layer : 노드 1024개\nmodel2.add( keras.layers.Dense(1024, activation = \"relu\"))\n\n# BatchNormalization\n\nmodel2.add(keras.layers.BatchNormalization())\n\n# DropOut : 35% 비활성화\n\nmodel2.add( keras.layers.Dropout(0.35) )\n\n# 아웃풋레이어\nmodel2.add( keras.layers.Dense(10, activation = \"softmax\"))\n\nmodel2.compile(optimizer = \"adam\",\n                              loss = keras.losses.categorical_crossentropy,\n                              metrics = [\"accuracy\"])\nmodel2.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 28, 28, 28)        280       \n                                                                 \n conv2d_1 (Conv2D)           (None, 28, 28, 28)        7084      \n                                                                 \n batch_normalization (Batch  (None, 28, 28, 28)        112       \n Normalization)                                                  \n                                                                 \n max_pooling2d (MaxPooling2  (None, 14, 14, 28)        0         \n D)                                                              \n                                                                 \n dropout (Dropout)           (None, 14, 14, 28)        0         \n                                                                 \n flatten (Flatten)           (None, 5488)              0         \n                                                                 \n dense (Dense)               (None, 1024)              5620736   \n                                                                 \n batch_normalization_1 (Bat  (None, 1024)              4096      \n chNormalization)                                                \n                                                                 \n dropout_1 (Dropout)         (None, 1024)              0         \n                                                                 \n dense_1 (Dense)             (None, 10)                10250     \n                                                                 \n=================================================================\nTotal params: 5642558 (21.52 MB)\nTrainable params: 5640454 (21.52 MB)\nNon-trainable params: 2104 (8.22 KB)\n_________________________________________________________________"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#모델-학습",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#모델-학습",
    "title": "04. MNIST",
    "section": "모델 학습",
    "text": "모델 학습\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\ntrain_x_rel.shape\n\n(60000, 28, 28, 1)\n\n\n\nes = EarlyStopping(\n      monitor = \"val_loss\",\n      min_delta = 0,\n      patience = 3,\n      verbose = 1,\n      restore_best_weights  = True\n)\n\n\nhistory = model2.fit(train_x_rel, train_y_c,\n                     epochs = 10000, verbose = 1,\n                     validation_split = 0.2,\n                     callbacks = [es]).history\n\nEpoch 1/10000\n1500/1500 [==============================] - 15s 7ms/step - loss: 0.4567 - accuracy: 0.8470 - val_loss: 0.2840 - val_accuracy: 0.9010\nEpoch 2/10000\n1500/1500 [==============================] - 11s 7ms/step - loss: 0.3097 - accuracy: 0.8893 - val_loss: 0.4721 - val_accuracy: 0.8445\nEpoch 3/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2647 - accuracy: 0.9051 - val_loss: 0.2349 - val_accuracy: 0.9137\nEpoch 4/10000\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.2322 - accuracy: 0.9162 - val_loss: 0.2548 - val_accuracy: 0.9100\nEpoch 5/10000\n1500/1500 [==============================] - 10s 6ms/step - loss: 0.2109 - accuracy: 0.9235 - val_loss: 0.2960 - val_accuracy: 0.8965\nEpoch 6/10000\n1496/1500 [============================&gt;.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9339Restoring model weights from the end of the best epoch: 3.\n1500/1500 [==============================] - 10s 7ms/step - loss: 0.1826 - accuracy: 0.9339 - val_loss: 0.2662 - val_accuracy: 0.9133\nEpoch 6: early stopping"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#결과-시각화-1",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#결과-시각화-1",
    "title": "04. MNIST",
    "section": "결과 시각화",
    "text": "결과 시각화\n\nplt.figure(figsize = (12,4))\nplt.plot(history[\"loss\"],\"--.\",label = \"train_loss\")\nplt.plot(history[\"val_loss\"], \"--.\",label = \"val_loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/advanced/2024-01-08-04. MNIST.html#예측-1",
    "href": "posts/advanced/2024-01-08-04. MNIST.html#예측-1",
    "title": "04. MNIST",
    "section": "예측",
    "text": "예측\n\ny_pred =  model1.predict(test_x_rel).argmax(axis = 1)\n\n313/313 [==============================] - 1s 3ms/step\n\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.88      0.84      1000\n           1       1.00      0.96      0.98      1000\n           2       0.74      0.84      0.79      1000\n           3       0.85      0.92      0.89      1000\n           4       0.84      0.72      0.78      1000\n           5       0.93      0.99      0.96      1000\n           6       0.74      0.64      0.69      1000\n           7       0.96      0.92      0.94      1000\n           8       0.96      0.97      0.97      1000\n           9       0.97      0.95      0.96      1000\n\n    accuracy                           0.88     10000\n   macro avg       0.88      0.88      0.88     10000\nweighted avg       0.88      0.88      0.88     10000"
  },
  {
    "objectID": "posts/advanced/2024-01-22-01. 텍스트 분류.html",
    "href": "posts/advanced/2024-01-22-01. 텍스트 분류.html",
    "title": "01. 텍스트 분류",
    "section": "",
    "text": "- tensorflow 튜토리얼 : 영화 리뷰를 사용한 텍스트 분류\n- environment : [colab]"
  },
  {
    "objectID": "posts/advanced/2024-01-22-01. 텍스트 분류.html#리뷰-길이-분포-확인",
    "href": "posts/advanced/2024-01-22-01. 텍스트 분류.html#리뷰-길이-분포-확인",
    "title": "01. 텍스트 분류",
    "section": "리뷰 길이 분포 확인",
    "text": "리뷰 길이 분포 확인\n- train 리뷰 길이 분포 시각화\n\nimport numpy as np\n\n\nreview_length = [len(i) for i in x_train]\n\nfig,axes = plt.subplots(1,2, figsize = (12,4))\n\nax1, ax2 = axes\n\nax1.hist(review_length)\n#ax1.set_title(\"리뷰길이 hist\")\nax2.boxplot(review_length)\n#ax2.set_title(\"리뷰길이 boxplot\")\nfig.tight_layout()\nplt.show()\n\nprint(f\"리뷰의 최대 길이  : {max(review_length)}\")\nprint(f\"리뷰의 평균 길이  : {np.mean(review_length)}\")\n\n\n\n\n리뷰의 최대 길이  : 2494\n리뷰의 평균 길이  : 238.71364"
  },
  {
    "objectID": "posts/advanced/2024-01-22-01. 텍스트 분류.html#라벨-분포-확인",
    "href": "posts/advanced/2024-01-22-01. 텍스트 분류.html#라벨-분포-확인",
    "title": "01. 텍스트 분류",
    "section": "라벨 분포 확인",
    "text": "라벨 분포 확인\n\nindex = np.unique(y_train, return_counts=True)[0]\ncounts = np.unique(y_train, return_counts=True)[1]\n\nnp.asarray((index, counts))\n\narray([[    0,     1],\n       [12500, 12500]])"
  },
  {
    "objectID": "posts/advanced/2024-01-22-01. 텍스트 분류.html#정수-맵핑-단어-확인",
    "href": "posts/advanced/2024-01-22-01. 텍스트 분류.html#정수-맵핑-단어-확인",
    "title": "01. 텍스트 분류",
    "section": "정수 맵핑 단어 확인",
    "text": "정수 맵핑 단어 확인\n- imdb.get_word_index()에 각 단어와 맵핑되는 정수가 저장되어 있음\n\n주의 : 저장된 값에 +3을 해야 실제 맵핑되는 정수임 (이것은 IMDB 리뷰 데이터 셋에서 정한 규칙이다.)\n\n\nword_to_index = imdb.get_word_index()\n#word_to_index\n\n\nindex_to_word = {}\n\nfor key, value in word_to_index.items() :\n    index_to_word[value+3] = key\n\n\nindex_to_word에 인덱스를 집어넣으면 전처리 전에 어떤 단어였는지 확인할 수 있음.\n\n0,1,2,3은 특별토큰, 정수 4부터가 실제 IMDB 리뷰 데이터셋에서 내림차순으로 빈도수가 영단어임\n\n\n\nprint(f\"빈도수 상위 1등 단어 : {index_to_word[4]}\" )\n\n빈도수 상위 1등 단어 : the\n\n\n\nprint(f\"빈도수 상위 3938등 단어 : {index_to_word[3941]}\")\n\n빈도수 상위 3938등 단어 : suited\n\n\n- 첫 번째 훈련용 리뷰의 각 단어가 정수로 바뀌기 전에 어떤 단어들이 었는지 확인\n\n아래작업은 0,1,2,3 특별토큰의 값을 집어넣는 과정\n\n\nfor index, token in enumerate((\"&lt;pad&gt;\", \"&lt;sos&gt;\", \"&lt;unk&gt;\")):\n  index_to_word[index] = token\n\n\nindex_to_word[0], index_to_word[1], index_to_word[2]\n\n('&lt;pad&gt;', '&lt;sos&gt;', '&lt;unk&gt;')\n\n\n\nprint(\" \".join([index_to_word[i] for i in x_train[0]]))\n\n&lt;sos&gt; this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all"
  },
  {
    "objectID": "posts/advanced/2024-01-22-01. 텍스트 분류.html#loss-시각화",
    "href": "posts/advanced/2024-01-22-01. 텍스트 분류.html#loss-시각화",
    "title": "01. 텍스트 분류",
    "section": "loss 시각화",
    "text": "loss 시각화\n\nplt.figure(figsize = (8,4))\nplt.plot(history1[\"loss\"], label = \"train_loss\")\nplt.plot(history1[\"val_loss\"], label = \"val_loss\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/advanced/2024-01-22-01. 텍스트 분류.html#predict",
    "href": "posts/advanced/2024-01-22-01. 텍스트 분류.html#predict",
    "title": "01. 텍스트 분류",
    "section": "predict",
    "text": "predict\n\nrnn_pred1 =  model.predict(x_test)\n\n782/782 [==============================] - 3s 3ms/step\n\n\n\nrnn_pred1 = np.where(rnn_pred1&gt;0.5,1,0)\n\n\nfrom sklearn.metrics import *\n\n\nprint(confusion_matrix(y_test, rnn_pred1))\nprint(classification_report(y_test, rnn_pred1))\n\n[[8756 3744]\n [3500 9000]]\n              precision    recall  f1-score   support\n\n           0       0.71      0.70      0.71     12500\n           1       0.71      0.72      0.71     12500\n\n    accuracy                           0.71     25000\n   macro avg       0.71      0.71      0.71     25000\nweighted avg       0.71      0.71      0.71     25000"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html",
    "href": "posts/basic/2023-10-30-00. Churn.html",
    "title": "00. Churn",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport scipy.stats as spst\nimport seaborn as sns"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#드라이브-마운트",
    "href": "posts/basic/2023-10-30-00. Churn.html#드라이브-마운트",
    "title": "00. Churn",
    "section": "드라이브 마운트",
    "text": "드라이브 마운트\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\ncd /content/drive/MyDrive/Colab Notebooks/DX/AICE\n\n/content/drive/MyDrive/Colab Notebooks/DX/AICE"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#데이터-로드",
    "href": "posts/basic/2023-10-30-00. Churn.html#데이터-로드",
    "title": "00. Churn",
    "section": "데이터 로드",
    "text": "데이터 로드\n\ndf = pd.read_csv(\"data_v1.csv\")\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\ncustomerID\ngender\nSeniorCitizen\nPartner\nDependents\ntenure\nPhoneService\nMultipleLines\nInternetService\nOnlineSecurity\n...\nDeviceProtection\nTechSupport\nStreamingTV\nStreamingMovies\nContract\nPaperlessBilling\nPaymentMethod\nMonthlyCharges\nTotalCharges\nChurn\n\n\n\n\n0\n7590-VHVEG\nNaN\n0.0\nYes\nNo\n1\nNo\nNo phone service\nDSL\nNo\n...\nNo\nNo\nNo\nNo\nNaN\nYes\nElectronic check\n29.85\n29.85\nNo\n\n\n1\n5575-GNVDE\nMale\n0.0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\n...\nYes\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\n\n\n2\n3668-QPYBK\nMale\n0.0\nNo\nNo\n2\nYes\nNo\nDSL\nYes\n...\nNaN\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n108.15\nYes\n\n\n3\n7795-CFOCW\nMale\n0.0\nNo\nNo\n45\nNo\nNo phone service\nDSL\nYes\n...\nNaN\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.30\n1840.75\nNo\n\n\n4\n9237-HQITU\nFemale\n0.0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\n...\nNaN\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.70\n151.65\nYes\n\n\n\n\n\n5 rows × 21 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7043 entries, 0 to 7042\nData columns (total 21 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   customerID        7043 non-null   object \n 1   gender            7034 non-null   object \n 2   SeniorCitizen     7042 non-null   float64\n 3   Partner           7043 non-null   object \n 4   Dependents        7041 non-null   object \n 5   tenure            7043 non-null   int64  \n 6   PhoneService      7040 non-null   object \n 7   MultipleLines     7043 non-null   object \n 8   InternetService   7043 non-null   object \n 9   OnlineSecurity    7043 non-null   object \n 10  OnlineBackup      7043 non-null   object \n 11  DeviceProtection  3580 non-null   object \n 12  TechSupport       7043 non-null   object \n 13  StreamingTV       7043 non-null   object \n 14  StreamingMovies   7043 non-null   object \n 15  Contract          7042 non-null   object \n 16  PaperlessBilling  7043 non-null   object \n 17  PaymentMethod     7042 non-null   object \n 18  MonthlyCharges    7042 non-null   float64\n 19  TotalCharges      7043 non-null   object \n 20  Churn             7043 non-null   object \ndtypes: float64(2), int64(1), object(18)\nmemory usage: 1.1+ MB"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#eda-exploratory-data-analysis",
    "href": "posts/basic/2023-10-30-00. Churn.html#eda-exploratory-data-analysis",
    "title": "00. Churn",
    "section": "EDA (Exploratory Data Analysis)",
    "text": "EDA (Exploratory Data Analysis)\n\n결측치 확인\n\ndf.isnull().sum()\n\ncustomerID             0\ngender                 9\nSeniorCitizen          1\nPartner                0\nDependents             2\ntenure                 0\nPhoneService           3\nMultipleLines          0\nInternetService        0\nOnlineSecurity         0\nOnlineBackup           0\nDeviceProtection    3463\nTechSupport            0\nStreamingTV            0\nStreamingMovies        0\nContract               1\nPaperlessBilling       0\nPaymentMethod          1\nMonthlyCharges         1\nTotalCharges           0\nChurn                  0\ndtype: int64\n\n\n\n\n통계 정보 확인\n\ndf.describe().T\n\n\n  \n    \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSeniorCitizen\n7042.0\n0.162170\n0.368633\n0.00\n0.0\n0.00\n0.00\n1.00\n\n\ntenure\n7043.0\n32.371149\n24.559481\n0.00\n9.0\n29.00\n55.00\n72.00\n\n\nMonthlyCharges\n7042.0\n64.763256\n30.091898\n18.25\n35.5\n70.35\n89.85\n118.75\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n전처리 수행\n\ncustomerID 컬럼을 삭제\n\n\ndf.drop(\"customerID\", axis = 1, inplace = True)\n\n\n확인\n\n\n\"customreID\" in df.columns\n\nFalse\n\n\n\nTotalCharges 컬럼의 타입을 object에서 float으로 변환\n\n\n[float(i)   for i in  df.TotalCharges][:5] ##\n\nValueError: ignored\n\n\n\n공백이 있어 error가 뜬다.\n\n\ndf.TotalCharges =  df.TotalCharges.replace(\" \", 0).replace(\"\",0)\n\n\ndf.TotalCharges = [float(i)   for i in  df.TotalCharges]\n\n\ndf.TotalCharges ## 실수형으로 바뀐 것을 확인\n\n0         29.85\n1       1889.50\n2        108.15\n3       1840.75\n4        151.65\n         ...   \n7038    1990.50\n7039    7362.90\n7040     346.45\n7041     306.60\n7042    6844.50\nName: TotalCharges, Length: 7043, dtype: float64\n\n\n- Churn 컬럼의 문자열 값을 숫자로 변경\n\ndf.Churn = df.Churn.replace([\"Yes\",\"No\"],[1,0])\ndf.Churn.dtypes\n\ndtype('int64')\n\n\n- 컬럼별로 null값이 얼마나 있는지 확인\n\ndf.isnull().sum()[df.isnull().sum() !=0]\n\ngender                 9\nSeniorCitizen          1\nDependents             2\nPhoneService           3\nDeviceProtection    3463\nContract               1\nPaymentMethod          1\nMonthlyCharges         1\ndtype: int64\n\n\n- 결측치가 많은 DeviceProtection 열을 제거\n\ndf.drop(\"DeviceProtection\", axis = 1, inplace = True)\ndf.isnull().sum()[df.isnull().sum() !=0]\n\ngender            9\nSeniorCitizen     1\nDependents        2\nPhoneService      3\nContract          1\nPaymentMethod     1\nMonthlyCharges    1\ndtype: int64\n\n\n\n\n시각화\n\nObject 컬럼을 하나씩 가져와서 Bar chart 그리기\n\n\no_col = df.select_dtypes(\"O\").columns.values\nlen(o_col)\n\n14\n\n\n\nfig, axes = plt.subplots(7, 2, figsize = (14, 20))\no_col = o_col.reshape(7,-1)\n\nfor i in range(7) :\n  for j in range(2) :\n      sns.countplot(x = o_col[i][j], data = df, ax = axes[i][j])\n      axes[i][j].set_title(o_col[i][j])\nfig.tight_layout()\n\n\n\n\n\n\n불균형이 심한 열(PhoneService)삭제\n\ndf.drop(\"PhoneService\", axis = 1, inplace = True)\n\"PhoneService\"  in  df.columns\n\nFalse\n\n\n\n\ntarget 변수 분포 파악\n\nsns.countplot(x = \"Churn\", data = df)\nplt.title(\"Churn\")\n\nText(0.5, 1.0, 'Churn')\n\n\n\n\n\n- target 변수의 분포 확인 결과 불균형을 보인다.\n\n\nSeniorCitizen 열 분포 확인\n\nsns.countplot(x = \"SeniorCitizen\", data = df)\nplt.title(\"SeniorCitizen\")\nplt.show()\n\n\n\n\n- 불균형이 심하므로 삭제\n\ndf.drop(\"SeniorCitizen\", axis = 1, inplace = True)\n\"SeniorCitizen\"  in  df.columns\n\nFalse\n\n\n\n\ntenure 열의 분포 확인\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 4))\nax1, ax2 = axes\n\nsns.histplot(x= \"tenure\", data = df, hue = \"Churn\", ax = ax1)\nsns.kdeplot(x= \"tenure\", data = df, hue = \"Churn\", ax = ax2)\n\nfig.tight_layout()\nfig.show()\n\n\n\n\n\n\nTotalCharges 열의 분포 확인\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 4))\nax1, ax2 = axes\n\nsns.histplot(x= \"TotalCharges\", data = df, hue = \"Churn\", ax = ax1)\nsns.kdeplot(x= \"TotalCharges\", data = df, hue = \"Churn\", ax = ax2)\n\nfig.tight_layout()\nfig.show()\n\n\n\n\n### MultipleLines 분포 확인\n\nsns.countplot(x = \"MultipleLines\", data = df, hue = \"Churn\")\nplt.show()\n\n\n\n\n\n\n상관관계 분석\n\ntenure, MonthlyCharges, TotalCharges 컬럼간의 상관관계를 heatmap으로 그려보시오.\n\n\nc_col = ['tenure', 'MonthlyCharges', 'TotalCharges']\n\n\nsns.heatmap(df[c_col].corr(),\n                        annot = True,\n                        fmt = \".2f\")\nfig.show()\n\n\n\n\n\ndf[c_col].corr()\n\n\n  \n    \n\n\n\n\n\n\ntenure\nMonthlyCharges\nTotalCharges\n\n\n\n\ntenure\n1.000000\n0.247871\n0.826178\n\n\nMonthlyCharges\n0.247871\n1.000000\n0.651167\n\n\nTotalCharges\n0.826178\n0.651167\n1.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nTotalCharges boxplot\n\nsns.boxplot(x = \"Churn\", y= \"TotalCharges\", data = df)\nplt.show()\n\n\n\n\n\n\n결과저장\n\ndf.to_csv(\"data_v1_save.csv\", index = False)\n\n\n\n다시 불러와서 확인\n\npd.read_csv(\"data_v1_save.csv\").shape\n\n(7043, 17)"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#더미변수-변환",
    "href": "posts/basic/2023-10-30-00. Churn.html#더미변수-변환",
    "title": "00. Churn",
    "section": "더미변수 변환",
    "text": "더미변수 변환\n\ndf = pd.read_csv(\"data_v1_save.csv\")\n\n\ndf = df.dropna()\n\n\nd_col = df.select_dtypes(\"O\").columns.values\nd_col\n\narray(['gender', 'Partner', 'Dependents', 'MultipleLines',\n       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod'], dtype=object)\n\n\n\ndf1 = pd.get_dummies(data = df, columns = d_col, drop_first = True)\ndf1.select_dtypes(\"O\").columns.values\n\narray([], dtype=object)\n\n\n\ndf1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 7030 entries, 1 to 7041\nData columns (total 27 columns):\n #   Column                                 Non-Null Count  Dtype  \n---  ------                                 --------------  -----  \n 0   tenure                                 7030 non-null   int64  \n 1   MonthlyCharges                         7030 non-null   float64\n 2   TotalCharges                           7030 non-null   float64\n 3   Churn                                  7030 non-null   int64  \n 4   gender_Male                            7030 non-null   uint8  \n 5   Partner_Yes                            7030 non-null   uint8  \n 6   Dependents_Yes                         7030 non-null   uint8  \n 7   MultipleLines_No phone service         7030 non-null   uint8  \n 8   MultipleLines_Yes                      7030 non-null   uint8  \n 9   InternetService_Fiber optic            7030 non-null   uint8  \n 10  InternetService_No                     7030 non-null   uint8  \n 11  OnlineSecurity_No internet service     7030 non-null   uint8  \n 12  OnlineSecurity_Yes                     7030 non-null   uint8  \n 13  OnlineBackup_No internet service       7030 non-null   uint8  \n 14  OnlineBackup_Yes                       7030 non-null   uint8  \n 15  TechSupport_No internet service        7030 non-null   uint8  \n 16  TechSupport_Yes                        7030 non-null   uint8  \n 17  StreamingTV_No internet service        7030 non-null   uint8  \n 18  StreamingTV_Yes                        7030 non-null   uint8  \n 19  StreamingMovies_No internet service    7030 non-null   uint8  \n 20  StreamingMovies_Yes                    7030 non-null   uint8  \n 21  Contract_One year                      7030 non-null   uint8  \n 22  Contract_Two year                      7030 non-null   uint8  \n 23  PaperlessBilling_Yes                   7030 non-null   uint8  \n 24  PaymentMethod_Credit card (automatic)  7030 non-null   uint8  \n 25  PaymentMethod_Electronic check         7030 non-null   uint8  \n 26  PaymentMethod_Mailed check             7030 non-null   uint8  \ndtypes: float64(2), int64(2), uint8(23)\nmemory usage: 432.5 KB\n\n\n\ndf1.to_csv(\"전처리완료.csv\",index= False)"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#훈련-평가-데이터-분리",
    "href": "posts/basic/2023-10-30-00. Churn.html#훈련-평가-데이터-분리",
    "title": "00. Churn",
    "section": "훈련, 평가 데이터 분리",
    "text": "훈련, 평가 데이터 분리\n\ntarget = \"Churn\"\n\nX = df1.drop(target, axis = 1)\ny = df1[target]\n\n\n# 입력 : X, y\n# Train : Test 비율 = 7:3\n# y Class 비율을 유지하면서 나누기 : stratify=y\n# 여러 번 수행해도 같은 결과 나오게 고정 : random_state=42\n# 결과 : X_train, X_test, y_train, y_test\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.3, stratify = y, random_state = 42 )\n\n\nX_train.shape\n\n(4921, 26)\n\n\n\ny_train.shape\n\n(4921,)"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#정규화스케일링",
    "href": "posts/basic/2023-10-30-00. Churn.html#정규화스케일링",
    "title": "00. Churn",
    "section": "정규화/스케일링",
    "text": "정규화/스케일링\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#모델별-bar차트-그려주고-성능-시각화-시험-x",
    "href": "posts/basic/2023-10-30-00. Churn.html#모델별-bar차트-그려주고-성능-시각화-시험-x",
    "title": "00. Churn",
    "section": "모델별 bar차트 그려주고 성능 시각화 (시험 X)",
    "text": "모델별 bar차트 그려주고 성능 시각화 (시험 X)\n\n\nCode\n# 모델별로 Recall 점수 저장\n# 모델 Recall 점수 순서대로 바차트를 그려 모델별로 성능 확인 가능\n\nfrom sklearn.metrics import accuracy_score\n\nmy_predictions = {}\n\ncolors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',\n          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',\n          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive',\n          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',\n          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray',\n          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'\n         ]\n\n# 모델명, 예측값, 실제값을 주면 위의 plot_predictions 함수 호출하여 Scatter 그래프 그리며\n# 모델별 MSE값을 Bar chart로 그려줌\n\ndef recall_eval(name_, pred, actual):\n    global predictions\n    global colors\n\n    plt.figure(figsize=(12, 9))\n\n    #acc = accuracy_score(actual, pred)\n    acc = recall_score(actual, pred)\n    my_predictions[name_] = acc * 100\n\n    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=True)\n\n    df = pd.DataFrame(y_value, columns=['model', 'recall'])\n    print(df)\n\n    length = len(df)\n\n    plt.figure(figsize=(10, length))\n    ax = plt.subplot()\n    ax.set_yticks(np.arange(len(df)))\n    ax.set_yticklabels(df['model'], fontsize=15)\n    bars = ax.barh(np.arange(len(df)), df['recall'])\n\n    for i, v in enumerate(df['recall']):\n        idx = np.random.choice(len(colors))\n        bars[i].set_color(colors[idx])\n        ax.text(v + 2, i, str(round(v, 3)), color='k', fontsize=15, fontweight='bold')\n\n    plt.title('recall', fontsize=18)\n    plt.xlim(0, 100)\n\n    plt.show()"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#모델-성능-평가",
    "href": "posts/basic/2023-10-30-00. Churn.html#모델-성능-평가",
    "title": "00. Churn",
    "section": "모델 성능 평가",
    "text": "모델 성능 평가\n\n1. 로지스틱\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nl_pred = model.predict(X_test)\n\n- 결과 확인\n\nfrom sklearn.metrics import *\n\n\nprint(confusion_matrix(y_test, l_pred))\nprint(classification_report(y_test, l_pred))\n\n[[1385  164]\n [ 246  314]]\n              precision    recall  f1-score   support\n\n           0       0.85      0.89      0.87      1549\n           1       0.66      0.56      0.61       560\n\n    accuracy                           0.81      2109\n   macro avg       0.75      0.73      0.74      2109\nweighted avg       0.80      0.81      0.80      2109\n\n\n\n\nrecall_eval(\"Logistic Regression\", l_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n\n\n2. KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel  = KNeighborsClassifier(n_neighbors = 5)\n\nmodel.fit(X_train, y_train)\n\nk_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, k_pred))\nprint(classification_report(y_test, k_pred))\n\n[[1310  239]\n [ 273  287]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.85      0.84      1549\n           1       0.55      0.51      0.53       560\n\n    accuracy                           0.76      2109\n   macro avg       0.69      0.68      0.68      2109\nweighted avg       0.75      0.76      0.75      2109\n\n\n\n\nrecall_eval(\"KNN\", k_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1                  KNN  51.250000\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n\n\n3. Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n\nmodel.fit(X_train, y_train)\n\ntree_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, tree_pred))\nprint(classification_report(y_test, tree_pred))\n\n[[1289  260]\n [ 265  295]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.83      0.83      1549\n           1       0.53      0.53      0.53       560\n\n    accuracy                           0.75      2109\n   macro avg       0.68      0.68      0.68      2109\nweighted avg       0.75      0.75      0.75      2109\n\n\n\n\nrecall_eval(\"Decision Tree\", tree_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n주요 Hyperparameter ##### 파라미터들을 조절하면서 모델의 성능을 높이거나 과대적합/과소적합 문제를 해결합니다.\n\nrandom_state: 랜덤 시드 고정 값. 고정해두고 튜닝하세요!\nn_jobs: CPU 사용 갯수 (여러 코어를 사용하면 모델 학습이 빨라짐)\nmax_depth: 깊어질 수 있는 최대 깊이. 너무 깊은 트리는 과대적합 발생할 수 있음\nn_estimators: 앙상블하는 트리의 갯수\nmax_features: 최대로 사용할 feature의 갯수. 값이 작을수록 과대적합 방지함\nmin_samples_split: 트리가 분할할 때 필요한 최소 샘플 수. 이 값을 증가시키면 각 분할에 샘플이 많이 필요해서 과대적합 방지함\n\n\n\n4. RandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators = 3, random_state = 42)\n\nmodel.fit(X_train, y_train)\n\nrf_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))\n\n[[1332  217]\n [ 300  260]]\n              precision    recall  f1-score   support\n\n           0       0.82      0.86      0.84      1549\n           1       0.55      0.46      0.50       560\n\n    accuracy                           0.75      2109\n   macro avg       0.68      0.66      0.67      2109\nweighted avg       0.74      0.75      0.75      2109\n\n\n\n\nrecall_eval(\"Random Forest\", rf_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n3        Random Forest  46.428571\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n주요 Hyperparameter - random_state: 랜덤 시드 고정 값. 고정해두고 튜닝하세요! - n_jobs: CPU 사용 갯수 (여러 코어를 사용하면 모델 학습이 빨라짐) - learning_rate: 학습율. 너무 큰 학습율은 성능이 떨어질 수 있고 너무 낮으면 학습이 느려져서 적절한 값을 찾아야 함, default=0.1 - n_estimators: 부스팅 스테이지 수. (랜덤포레스트 트리의 갯수 설정과 비슷한 개념). default=100 - max_depth: 트리의 깊이. 너무 높으면 과적합, 너무 낮으면 성능이 떨어짐. default=3. - subsample: 샘플 사용 비율(0~1 사이의 값), 과대적합 방지용. - max_features: 최대로 사용할 feature의 비율. 과대적합 방지용. default=1.0\n\n\n5. XGboost\n\n#!pip install xgboost\n\n\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier()\n\n\nmodel.fit(X_train, y_train)\n\nx_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, x_pred))\nprint(classification_report(y_test, x_pred))\n\n[[1359  190]\n [ 278  282]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.88      0.85      1549\n           1       0.60      0.50      0.55       560\n\n    accuracy                           0.78      2109\n   macro avg       0.71      0.69      0.70      2109\nweighted avg       0.77      0.78      0.77      2109\n\n\n\n\nrecall_eval(\"XGboost\", x_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n3              XGboost  50.357143\n4        Random Forest  46.428571\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;\n\n\n\n\n\n주요 Hyperparameter - random_state: 랜덤 시드 고정 값. 고정해두고 튜닝하세요! - n_jobs: CPU 사용 갯수 (여러 코어를 사용하면 모델 학습이 빨라짐) - learning_rate: 학습율. 이 값이 너무 높으면 과적합할 수 있고 낮으면 학습이 느려질 수 있음. 적절한 값을 찾아야함. default=0.1 - n_estimators: 부스팅 스테이지 수.(랜덤포레스트 트리의 갯수와 비슷한 개념). 높을수록 복잡성 증가함. default=100 - max_depth: 트리의 깊이. 값이 크면 과적합 위험이 있음. default=3. - colsample_bytree: 샘플 사용 비율 (max_features와 비슷한 개념). 과대적합 방지용. default=1.0\n\n\n6. LGBM\n\n#!pip install lightgbm\n\n\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(n_estimator = 3, random_state = 42)\n\nmodel.fit(X_train, y_train)\n\nlg_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, lg_pred))\nprint(classification_report(y_test, lg_pred))\n\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[LightGBM] [Info] Number of positive: 1308, number of negative: 3613\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 629\n[LightGBM] [Info] Number of data points in the train set: 4921, number of used features: 26\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.265800 -&gt; initscore=-1.016039\n[LightGBM] [Info] Start training from score -1.016039\n[LightGBM] [Warning] Unknown parameter: n_estimator\n[[1377  172]\n [ 273  287]]\n              precision    recall  f1-score   support\n\n           0       0.83      0.89      0.86      1549\n           1       0.63      0.51      0.56       560\n\n    accuracy                           0.79      2109\n   macro avg       0.73      0.70      0.71      2109\nweighted avg       0.78      0.79      0.78      2109\n\n\n\n\nrecall_eval('LGBM', lg_pred, y_test)\n\n                 model     recall\n0  Logistic Regression  56.071429\n1        Decision Tree  52.678571\n2                  KNN  51.250000\n3                 LGBM  51.250000\n4              XGboost  50.357143\n5        Random Forest  46.428571\n\n\n&lt;Figure size 1200x900 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#데이터-셋-분리",
    "href": "posts/basic/2023-10-30-00. Churn.html#데이터-셋-분리",
    "title": "00. Churn",
    "section": "데이터 셋 분리",
    "text": "데이터 셋 분리\n\ntarget = \"Churn\"\n\nX = df.drop(target, axis = 1)\ny = df[target]\n\n\nfrom sklearn.model_selection import train_test_split\n\n\n# 입력 : X, y\n# Train : Test 비율 = 7:3\n# y Class 비율을 유지하면서 나누기 : stratify=y\n# 여러 번 수행해도 같은 결과 나오게 고정 : random_state=42\n# 결과 : X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 42)\n\n\n스케일링\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#dnn-구현-1",
    "href": "posts/basic/2023-10-30-00. Churn.html#dnn-구현-1",
    "title": "00. Churn",
    "section": "DNN 구현 1",
    "text": "DNN 구현 1\n\nimport tensorflow as tf\n\n- 하이퍼 파라미터 설정\n\nbatch_size = 16\nepochs = 20\n\n- 아래의 요구대로 Sequential 모델 만들기\n\n# Sequential() 모델 정의 하고 model로 저장\n# input layer는 input_shape=() 옵션을 사용한다.\n# 39개 input layer\n# unit 4개 hidden layer\n# unit 3개 hidden layer\n# 1개 output layser : 이진분류\n\n\nX_train.shape\n\n(4921, 26)\n\n\n\nnf = X_train.shape[1]\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(4, activation = \"relu\", input_shape = (nf,)))\nmodel.add(tf.keras.layers.Dense(3, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))\n\n\nmodel.summary()\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_12 (Dense)            (None, 4)                 108       \n                                                                 \n dense_13 (Dense)            (None, 3)                 15        \n                                                                 \n dense_14 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 127 (508.00 Byte)\nTrainable params: 127 (508.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- dropout 추가\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(4, activation = \"relu\", input_shape = (nf,)))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(3, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))\n\n\nmodel.summary()\n\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_15 (Dense)            (None, 4)                 108       \n                                                                 \n dropout (Dropout)           (None, 4)                 0         \n                                                                 \n dense_16 (Dense)            (None, 3)                 15        \n                                                                 \n dropout_1 (Dropout)         (None, 3)                 0         \n                                                                 \n dense_17 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 127 (508.00 Byte)\nTrainable params: 127 (508.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n- 아래의 요구사항대로 모델을 컴파일\n\n# - 옵티마이저 : 'adam'\n# - 손실 함수 : 'binary_crossentropy'\n# - 평가 지표 : 'accuracy'\n\nmodel.compile(loss = tf.keras.losses.binary_crossentropy,\n                            optimizer = tf.keras.optimizers.Adam(0.001),\n                              metrics = [\"accuracy\"])\n\n\n아래 요구사항대로 모델 학습 시키기\n\n모델 이름 : model\nSequential 모델의 fit() 함수 사용\nX, y : X_train, y_train\nvalidation_data=(X_test, y_test)\nepochs : 10번\nbatch_size : 10번\n\n\nmodel.fit(X_train, y_train,\n                validation_data=(X_test, y_test),\n                epochs = 10,\n                batch_size = 10)\n\nEpoch 1/10\n493/493 [==============================] - 11s 5ms/step - loss: 0.6046 - accuracy: 0.7106 - val_loss: 0.5062 - val_accuracy: 0.7345\nEpoch 2/10\n493/493 [==============================] - 2s 5ms/step - loss: 0.5344 - accuracy: 0.7342 - val_loss: 0.4865 - val_accuracy: 0.7345\nEpoch 3/10\n493/493 [==============================] - 2s 5ms/step - loss: 0.5257 - accuracy: 0.7342 - val_loss: 0.4782 - val_accuracy: 0.7345\nEpoch 4/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.5206 - accuracy: 0.7342 - val_loss: 0.4726 - val_accuracy: 0.7345\nEpoch 5/10\n493/493 [==============================] - 3s 7ms/step - loss: 0.5192 - accuracy: 0.7368 - val_loss: 0.4628 - val_accuracy: 0.7387\nEpoch 6/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.5051 - accuracy: 0.7559 - val_loss: 0.4498 - val_accuracy: 0.7615\nEpoch 7/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.4985 - accuracy: 0.7586 - val_loss: 0.4547 - val_accuracy: 0.7520\nEpoch 8/10\n493/493 [==============================] - 2s 5ms/step - loss: 0.4960 - accuracy: 0.7663 - val_loss: 0.4525 - val_accuracy: 0.7596\nEpoch 9/10\n493/493 [==============================] - 3s 5ms/step - loss: 0.4952 - accuracy: 0.7551 - val_loss: 0.4512 - val_accuracy: 0.7525\nEpoch 10/10\n493/493 [==============================] - 5s 11ms/step - loss: 0.4868 - accuracy: 0.7663 - val_loss: 0.4487 - val_accuracy: 0.7577\n\n\n&lt;keras.src.callbacks.History at 0x780ac638ace0&gt;"
  },
  {
    "objectID": "posts/basic/2023-10-30-00. Churn.html#dnn-구현-2",
    "href": "posts/basic/2023-10-30-00. Churn.html#dnn-구현-2",
    "title": "00. Churn",
    "section": "DNN 구현 2",
    "text": "DNN 구현 2\n- 아래의 요구사항대로 모델을 설계\n\n# 39개 input layer\n# unit 5개 hidden layer\n# dropout\n# unit 4개 hidden layer\n# dropout\n# 2개 output layser : 다중분류\n# epochs : 20번\n# batch_size : 16번\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(5, activation = \"relu\", input_shape = (nf,)))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(4, activation  = \"relu\"))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\n\n\nmodel.summary()\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_18 (Dense)            (None, 5)                 135       \n                                                                 \n dropout_2 (Dropout)         (None, 5)                 0         \n                                                                 \n dense_19 (Dense)            (None, 4)                 24        \n                                                                 \n dropout_3 (Dropout)         (None, 4)                 0         \n                                                                 \n dense_20 (Dense)            (None, 2)                 10        \n                                                                 \n=================================================================\nTotal params: 169 (676.00 Byte)\nTrainable params: 169 (676.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n아래 요구사항대로 모델 학습 시키기\n\nmodel.compile(loss = tf.keras.losses.sparse_categorical_crossentropy,\n                            optimizer = tf.keras.optimizers.Adam(0.001), metrics = [\"accuracy\"])\n\n\n# 학습데이터 : X_train, y_train\n# 검증데이터 : X_test, y_test\n# epochs : 20, batch_size : 16\n# 'history' 변수에 저장\n\nhistory = model.fit(X_train, y_train,\n                                validation_data = [X_test, y_test],\n                                    epochs = 20, batch_size = 16).history\n\nEpoch 1/20\n308/308 [==============================] - 3s 5ms/step - loss: 0.5928 - accuracy: 0.7055 - val_loss: 0.5120 - val_accuracy: 0.7345\nEpoch 2/20\n308/308 [==============================] - 3s 11ms/step - loss: 0.5300 - accuracy: 0.7476 - val_loss: 0.4789 - val_accuracy: 0.7373\nEpoch 3/20\n308/308 [==============================] - 2s 6ms/step - loss: 0.5085 - accuracy: 0.7535 - val_loss: 0.4659 - val_accuracy: 0.7482\nEpoch 4/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4919 - accuracy: 0.7606 - val_loss: 0.4555 - val_accuracy: 0.7553\nEpoch 5/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4909 - accuracy: 0.7588 - val_loss: 0.4540 - val_accuracy: 0.7530\nEpoch 6/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4769 - accuracy: 0.7635 - val_loss: 0.4469 - val_accuracy: 0.7596\nEpoch 7/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4734 - accuracy: 0.7702 - val_loss: 0.4440 - val_accuracy: 0.7743\nEpoch 8/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4767 - accuracy: 0.7669 - val_loss: 0.4434 - val_accuracy: 0.7776\nEpoch 9/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4803 - accuracy: 0.7610 - val_loss: 0.4478 - val_accuracy: 0.7672\nEpoch 10/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4787 - accuracy: 0.7629 - val_loss: 0.4457 - val_accuracy: 0.7672\nEpoch 11/20\n308/308 [==============================] - 2s 7ms/step - loss: 0.4748 - accuracy: 0.7641 - val_loss: 0.4414 - val_accuracy: 0.7710\nEpoch 12/20\n308/308 [==============================] - 2s 7ms/step - loss: 0.4741 - accuracy: 0.7692 - val_loss: 0.4396 - val_accuracy: 0.7776\nEpoch 13/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4752 - accuracy: 0.7653 - val_loss: 0.4382 - val_accuracy: 0.7800\nEpoch 14/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4637 - accuracy: 0.7647 - val_loss: 0.4379 - val_accuracy: 0.7805\nEpoch 15/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4571 - accuracy: 0.7704 - val_loss: 0.4369 - val_accuracy: 0.7824\nEpoch 16/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4623 - accuracy: 0.7667 - val_loss: 0.4388 - val_accuracy: 0.7790\nEpoch 17/20\n308/308 [==============================] - 1s 4ms/step - loss: 0.4656 - accuracy: 0.7655 - val_loss: 0.4400 - val_accuracy: 0.7814\nEpoch 18/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4638 - accuracy: 0.7635 - val_loss: 0.4393 - val_accuracy: 0.7786\nEpoch 19/20\n308/308 [==============================] - 1s 5ms/step - loss: 0.4618 - accuracy: 0.7659 - val_loss: 0.4384 - val_accuracy: 0.7809\nEpoch 20/20\n308/308 [==============================] - 2s 6ms/step - loss: 0.4616 - accuracy: 0.7673 - val_loss: 0.4386 - val_accuracy: 0.7824\n\n\n\n\n조기종료 설정\n\n# val_loss(검증손실) 모니터링해서 손실이 최소화되는 방향으로 모니터링\n# 성능이 5번 지나도록 좋아지지 않으면 조기 종료\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",mode= \"min\",\n                                                                                  verbose = 1, patience = 5)\n\n\n# 최적의 모델을 'best_model.h5' 파일로 저장, 검증 손실이 최소일 때만 저장\n# val_loss 가장 낮은 값을 가질 때마다 모델 저장\n\ncheck_point = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", verbose = 1,\n                                                                monitors = \"val_loss\",mode = \"min\", save_best_only = True)\n\n\n\n학습\n\n# early_stop과 check_point Callback 사용하여 모델에 적용시키세요.\n# epochs=50, batch_size=20\n# 검증 데이터로 X_test, y_test 사용\n# verbose 옵션을 1로 설정\n# 'history' 변수에 저장\n\n\nhistory = model.fit(X_train, y_train,\n                                validation_data = (X_test, y_test),\n                                epochs = 50, batch_size = 20,\n                                callbacks = [early_stop, check_point], verbose = 1 ).history\n\nEpoch 1/50\n246/247 [============================&gt;.] - ETA: 0s - loss: 0.4575 - accuracy: 0.7770\nEpoch 1: val_loss improved from inf to 0.43708, saving model to best_model.h5\n247/247 [==============================] - 2s 10ms/step - loss: 0.4579 - accuracy: 0.7769 - val_loss: 0.4371 - val_accuracy: 0.7819\nEpoch 2/50\n 18/247 [=&gt;............................] - ETA: 1s - loss: 0.4827 - accuracy: 0.7278242/247 [============================&gt;.] - ETA: 0s - loss: 0.4659 - accuracy: 0.7616\nEpoch 2: val_loss improved from 0.43708 to 0.43677, saving model to best_model.h5\n247/247 [==============================] - 2s 9ms/step - loss: 0.4652 - accuracy: 0.7620 - val_loss: 0.4368 - val_accuracy: 0.7814\nEpoch 3/50\n240/247 [============================&gt;.] - ETA: 0s - loss: 0.4550 - accuracy: 0.7690\nEpoch 3: val_loss did not improve from 0.43677\n247/247 [==============================] - 2s 10ms/step - loss: 0.4568 - accuracy: 0.7673 - val_loss: 0.4374 - val_accuracy: 0.7881\nEpoch 4/50\n244/247 [============================&gt;.] - ETA: 0s - loss: 0.4572 - accuracy: 0.7736\nEpoch 4: val_loss improved from 0.43677 to 0.43675, saving model to best_model.h5\n247/247 [==============================] - 1s 6ms/step - loss: 0.4576 - accuracy: 0.7726 - val_loss: 0.4367 - val_accuracy: 0.7833\nEpoch 5/50\n243/247 [============================&gt;.] - ETA: 0s - loss: 0.4547 - accuracy: 0.7671\nEpoch 5: val_loss improved from 0.43675 to 0.43665, saving model to best_model.h5\n247/247 [==============================] - 1s 5ms/step - loss: 0.4559 - accuracy: 0.7659 - val_loss: 0.4366 - val_accuracy: 0.7805\nEpoch 6/50\n235/247 [===========================&gt;..] - ETA: 0s - loss: 0.4576 - accuracy: 0.7668\nEpoch 6: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 5ms/step - loss: 0.4577 - accuracy: 0.7669 - val_loss: 0.4382 - val_accuracy: 0.7795\nEpoch 7/50\n235/247 [===========================&gt;..] - ETA: 0s - loss: 0.4550 - accuracy: 0.7687\nEpoch 7: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 4ms/step - loss: 0.4531 - accuracy: 0.7700 - val_loss: 0.4376 - val_accuracy: 0.7847\nEpoch 8/50\n237/247 [===========================&gt;..] - ETA: 0s - loss: 0.4614 - accuracy: 0.7696\nEpoch 8: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 5ms/step - loss: 0.4627 - accuracy: 0.7683 - val_loss: 0.4390 - val_accuracy: 0.7819\nEpoch 9/50\n235/247 [===========================&gt;..] - ETA: 0s - loss: 0.4622 - accuracy: 0.7689\nEpoch 9: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 4ms/step - loss: 0.4599 - accuracy: 0.7708 - val_loss: 0.4379 - val_accuracy: 0.7881\nEpoch 10/50\n245/247 [============================&gt;.] - ETA: 0s - loss: 0.4555 - accuracy: 0.7684\nEpoch 10: val_loss did not improve from 0.43665\n247/247 [==============================] - 1s 5ms/step - loss: 0.4552 - accuracy: 0.7683 - val_loss: 0.4371 - val_accuracy: 0.7824\nEpoch 10: early stopping\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning:\n\nYou are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n\n\n\n\n\nloss 확인\n\nlosses = pd.DataFrame(history)\n\n\nlosses\n\n\n  \n    \n\n\n\n\n\n\nloss\naccuracy\nval_loss\nval_accuracy\n\n\n\n\n0\n0.457895\n0.776875\n0.437084\n0.781887\n\n\n1\n0.465190\n0.762040\n0.436769\n0.781413\n\n\n2\n0.456798\n0.767324\n0.437379\n0.788051\n\n\n3\n0.457556\n0.772607\n0.436749\n0.783310\n\n\n4\n0.455916\n0.765901\n0.436647\n0.780465\n\n\n5\n0.457705\n0.766917\n0.438182\n0.779516\n\n\n6\n0.453103\n0.769965\n0.437608\n0.784732\n\n\n7\n0.462745\n0.768340\n0.439002\n0.781887\n\n\n8\n0.459855\n0.770778\n0.437930\n0.788051\n\n\n9\n0.455196\n0.768340\n0.437104\n0.782361\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n성능 시각화\n\nlosses.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n아래 조건에 따라 정확도를 시각화하는 코드작성\n\n‘history’ 객체를 사용\n그래프 제목 : ‘Accuracy’\nx축 레이블 : ‘Epochs’\ny축 레이블 : ‘Acc’\n범례 : ‘acc’, ‘val_acc’\n\n\nplt.plot(losses[\"accuracy\"], label = \"acc\")\nplt.plot(losses[\"val_accuracy\"], label = \"val_acc\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Acc\")\nplt.title(\"Accuracy\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x780ab6edc970&gt;\n\n\n\n\n\n\n\n성능 평가\n\npred = model.predict(X_test).argmax(axis = 1)\n\n66/66 [==============================] - 0s 3ms/step\n\n\n\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test,pred))\n\n[[1447  102]\n [ 357  203]]\n              precision    recall  f1-score   support\n\n           0       0.80      0.93      0.86      1549\n           1       0.67      0.36      0.47       560\n\n    accuracy                           0.78      2109\n   macro avg       0.73      0.65      0.67      2109\nweighted avg       0.77      0.78      0.76      2109\n\n\n\n\n\n(참고) 재현율 성능이 좋지 않다면 어떻게 성능향상 할 수 있나?\n\n성능향상을 할 수 있는 방법은 여러 가지가 있습니다. (시험범위는 아닙니다)\nDNN 하이퍼파라미터를 수정하면서 성능향상이 되는지 확인해 볼 수 있습니다.\n데이터를 줄이거나(UnderSampling) 늘리거나(OverSampling), Feature(컬럼)을 늘리거나 줄이거나 하는 식으로 데이터를 균형하게 조정할 수도 있습니다.c"
  },
  {
    "objectID": "posts/basic/2023-11-01-02. iris.html",
    "href": "posts/basic/2023-11-01-02. iris.html",
    "title": "02. iris",
    "section": "",
    "text": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = \"Malgun Gothic\"\nplt.rcParams['axes.unicode_minus'] = False\n\n\niris = sns.load_dataset(\"iris\")\n\n\niris.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa"
  },
  {
    "objectID": "posts/basic/2023-11-01-02. iris.html#species-분포-확인",
    "href": "posts/basic/2023-11-01-02. iris.html#species-분포-확인",
    "title": "02. iris",
    "section": "(1) species 분포 확인",
    "text": "(1) species 분포 확인\n- barplot\n\nplt.figure(figsize = (4,4))\nsns.countplot(x = \"species\", data = iris )\nplt.show()\n\n\n\n\n- scatterplot\n\nsns.scatterplot(x = \"sepal_length\", y = \"petal_length\", hue = \"species\", data = iris)\n\n&lt;Axes: xlabel='sepal_length', ylabel='petal_length'&gt;"
  },
  {
    "objectID": "posts/basic/2023-11-01-02. iris.html#species-라벨-인코딩",
    "href": "posts/basic/2023-11-01-02. iris.html#species-라벨-인코딩",
    "title": "02. iris",
    "section": "(1) species 라벨 인코딩",
    "text": "(1) species 라벨 인코딩\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(le.classes_)\n\n['setosa' 'versicolor' 'virginica']"
  },
  {
    "objectID": "posts/basic/2023-11-01-02. iris.html#tree",
    "href": "posts/basic/2023-11-01-02. iris.html#tree",
    "title": "02. iris",
    "section": "(1) tree",
    "text": "(1) tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth=5)\n\ntree.fit(X, y)\n\ntree_pred = tree.predict(X)\n\nfrom sklearn.metrics import *\n\naccuracy_score(y, tree_pred)\n\n1.0"
  },
  {
    "objectID": "posts/basic/2023-11-01-02. iris.html#rf",
    "href": "posts/basic/2023-11-01-02. iris.html#rf",
    "title": "02. iris",
    "section": "(2) RF",
    "text": "(2) RF\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5, n_estimators = 10)\n\nrf.fit(X, y)\n\nrf_pred = rf.predict(X)\n\naccuracy_score(y, rf_pred)\n\n0.98\n\n\n\npredict 후 라벨 추출\n\nle.classes_\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nle.classes_[rf.predict([X[50]])[0]]\n\n'versicolor'"
  }
]