{"title":"Extra 01. summary (1)","markdown":{"yaml":{"title":"Extra 01. summary (1)","author":"GC","date":"01/21/23","categories":["python"]},"headingText":"Supervised learning","containsRefs":false,"markdown":"\n\n\n## 00. 모델링 단계\n\n### step1. 데이터 이해 및 준비\n\n`-` knn의 경우 필요하다면 스케일링 단계가 필요\n\n`-` 이산형 변수, 즉 범주형 변수를 모델의 예측변수로 사용할 경우 더미변수로 변환해주어야한다.\n\n```python\npd.get_dummies(data, columns = 더미화할컬럼리스트, dtype = (int or float))\n```\n\n`-` 결측치 처리 : 히스토그램, boxplot, 시계열 데이터인 경우 등등을 고려하여 각 case에 맞게 적절히 결측치를 처리해준다.\n\n* misforest, EM 알고리즘을 통한 결측치 처리를 한다지만, 개인적인 생각으로는 좀 과한 결측치 처리가 아닌지 싶음\n\n* 이유는 즉슨, 결측치를 처리하기위해 결측치 처리 단계에서 모델링을 한번 더 수행하는데 이 때 시간이 생각보다 오래 걸림\n\n### step2. cross-validation을 통한 최적의 모델 선택\n\n`-` 아래와 같이 여러개의 모델을 생성한다음 `cross-validation` 통해 최적의 모델을 선택하였다.\n\n> example \n\n\n```python\n\n# 1. knn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.cv = cross_val_score(knn, x_train_s, y_train, cv = 5)\n\nknn.cv_m = knn.cv.mean()\n\n# 2. tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth = 5, random_state = 1)\n\ntree.cv = cross_val_score(tree, x_train, y_train, cv = 5)\n\ntree.cv_m = tree.cv.mean()\n\n# 3. logistic\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.cv = cross_val_score(logit, x_train, y_train, cv = 5)\n\nlogit.cv_m = logit.cv.mean()\n\n# 4. RF\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5,  random_state = 1)\n\nrf.cv = cross_val_score(rf, x_train,y_train)\n\nrf.cv_m = rf.cv.mean()\n\n# 5. XGBoost\nfrom xgboost import XGBClassifier\nxgb =  XGBClassifier(max_depth = 5, random_state = 1)\n\nxgb.cv = cross_val_score(xgb, x_train, y_train, cv = 5)\n\nxgb.cv_m  = xgb.cv.mean()\n\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(max_depth = 5, random_state = 1,verbose = -100) \n\nlgbm.cv = cross_val_score(lgbm, x_train, y_train, cv = 5)\n\nlgbm.cv_m = lgbm.cv.mean()\n\n```\n\n### step 3. 모델 튜닝\n\n`-` 그 후 선택한 최종 모델을 튜닝해 최종 모델을 select\n\n```python\nfrom sklearn.model_selection import GridSearchCV\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nparams=  {\"max_depth\" : range(1,21)}\n\nmodel = GridSearchCV(model,\n                     params,\n                     cv=5,\n                     scoring='r2')\n\n```\n\n***\n\n## 01. 각 모델 소개\n\n### (1) regression model\n\n`-` 아래 링크를 참조해서 까먹을때 마다 보자~~\n\n`-` [ISLP2023-00.Linear Regression](https://gangcheol.github.io/ISLP2023/posts/2023-09-09-00.%20Linear%20Regression.html)\n\n### (2) 판별분석, 베이즈 분류기\n\n`-` 이것두 아래 링크를 참조하자.\n\n`-` [ISLP2023-01. Classification](https://gangcheol.github.io/ISLP2023/posts/2023-09-09-01.%20classification.html#%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9Ddiscriminant-analysis)\n\n### (3) KNN (K-nearest neighbors)\n\n`-` 학습을 안함, 그냥 그 근처 K개의 녀석들을 보고 값을 할당\n\n$$P(y= j | X = x_0) = \\frac {1}{K}  \\sum_{i\\in N_0} I(y_i = j)$$\n\n$$N_0  : x_0\\text {와 가장 가까운  K개의 자료의 집합}$$\n\n`-` $k$가 작을수록 모델은 복잡해지고, 클수록 단순해짐\n\n* 솔직히 와닿지 않지만, 내 방식대로 이해해보자.\n\n* 이전에 선형회귀분석에서 모델 복잡도를 생각해보자, 모델을 일직선으로 예측한 경우 단순선형회귀분석이다.\n\n* 즉, 모델 하나하나의 포인트를 고려하지 않고 전체 평균적인 선형회귀식을 하나 구한 것이다.\n\n* 이를 다시 KNN예제로 생각해 $K$가 클경우 생각해보변, 주변 녀석들의 하나하나 개인적은 특성을 고려하기보단 전체적인 특성에 기반하여 주어진$x_0$에 대한 $y$를 예측하는 것이다.\n\n* 따라서, $k$가 작을수록 주변 녀석들의 특징을 하나하나 잘 고려해서 모델이 복잡한 것이고, $k$가 크면 전체적인 평균을 고려한 것이기 때문에 모델이 단순해진다...\n$\\to$ 사실 이것도 그렇게 와닿지 않음 나중에 더 찾아보자...\n\n### (4) Decsion Tree\n\n>  나무모형은 간단하고 해석상에 장점이 있으나 다른 방법들에 비해 좋은 성능을 보이지 못하는 경우가 있음\n\n#### 형성단계\n\n`1`  설명변수들의 가능한 조합을 이용하여 예측공간을 $J$개의 겹치지 않는(non-overlaping)구역으로 분할\n\n`2` 각 관측값은 $R_j$ 구역에 포함되며, $R_j$ 구역에 포함된 training data의 반응변수 ($y$)의 평균  (분류문제에선 voting방식)을 이용하여 예측\n\n$$\\hat {y}_{R_j} = \\frac {1}{n_j} \\sum_{k\\in R_J} y_k $$\n\n`3` 목표 : 다음의 RSS를 최소화 하는 구역 $R_1,R_2, R_J$를 찾는 것\n\n$$RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat {y}_{R_{j}})^2$$\n\n`4` 모든 조합을 확인하는 것은 불가능...(사실 가능하다. tree를 무한정 쪼개면)\n\n* 근데  tree를 무한정 쪼갤경우 과적합문제가 무조건 발생\n\n`5`  정지규칙 (stopping rule)\n\n* 모든 자료가 한 범주에 속할 때\n\n* 노드에 속하는 자료가 일정 수 이하일 떄 \n\n* MSE의 감소량이 아주 작을 떄 \n\n* 뿌리마디로부터의 깊이가 일정 수 이상일 떄 등 (max_depth)\n\n`6` 가지치기 : 과적합을 막기위한 방법\n\n* 사실 정지규칙도 이에 포함됨, 따라서 위에거 + 빠진 내용을 적겠음\n\n*  **min_samples_leaf(default = 1)** : leaf노드가 되기 위한 최소한의 샘플 수\n\n*  **min_samples_split(default = 20** : 노드를 분할하기 위한 최소한의 샘플 수 (값을 적게 설정할수록 계순 분할되어, 과적합 발생 위험 증가)\n\n* **max_feature** : 최선의 분할을 위해 고려할 변수(feature) 개수\n\n    * sqrt : 전체 변수 개수의 루트\n    \n    * auto :  sqrt와 같은 의미\n    \n    * log : $\\log_{2}$ (전체 변수의 수)\n\n*  max_leaf_node : 리프 노드의 최대 개수 $\\to$ $\\text{Cost complexity Pruning}$\n\n     * $|T|$는 터미널도드로 리프노드의 개수를 뜻한다.\n     \n     *  $R_{\\alpha}(T)$는 변하지 않는 비용함수로 $R(T)$는 우리가 알고 있는 $RSS$와 같다.\n     \n     * 아래식이 뜻하는 바는  리프노드의 개수가 클수록  $R(T)$ 훈련 데이터 셋에대한 $RSS$가 작아져 과적합 문제가 발생할 수 있기 때문에 적절한 리프노드의 개수를 설정해야한다는 의미이다.\n     \n     * $\\alpha$는 $\\text {tuning parameter}$로 복잡도를 조절한다. 만약 $\\alpha$가 0이라면 기존의 비용함수와 같고 1에 가까워질수록 $R(T)$값이 작아진다.\n     \n     * 따라서 우리는 적절한 $\\alpha$값과 $|T|$값을 교차검증 기법을 통해 찾아내어 가지치기를 수행하여야한다.\n\n$$\\begin {align}R_{\\alpha}(T) &= \\sum R(T) + \\alpha |T|  \\\\ \\\\\n                                                          &= \\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat {y}_{R_m})^2 +  \\alpha |T|  \\end {align}$$\n\n`7` 비용함수\n\n(1) 지니 지수 (Gini Index)\n\n$$Gini (D) = 1- \\sum_{k=1}^{K}p_{k}^2 = \\sum_{k=1}^{K} p_k(1-p_k)$$\n\n$$p_k : \\text{Node D에서 k번째 범주에 속하는 관측 비율}$$\n\n<center><img src = \"GINI.png\" width= 500> </center>\n\n* 순수하게 분류되면 값은 0이다.\n\n* 만약 분리규칙 $A$에 의해서 **Node D**가 $D_1, D_2$로 분리된다면, 분리규칙 $A$에서 Ginin지수는 다음과 같다.\n\n$$Gini_A(D) =\\frac {|D_1|}{|D|}Gini(D_1) +\\frac {|D_2|}{|D|}Gini(D_2) $$\n\n* 위에 근거하여 분리규칙 A에서 발생한 **불순도 감소량**은 다음과 같이 정의할 수 있다.\n\n$$\\Delta Gini(A) = Gini(D) - Gini_{A}(D)$$\n\n* 따라서, $Gini_{A}(D)$를 가장 작게 하거나 $\\Delta Gini(A)$를 가장 크게 하는 분리 규칙을 선택!\n\n***\n\n(2)  엔트로피(Entropy)\n\n$$\\text {Entropy}  = -\\sum_{i=1}^m p_i\\log_{2} p_i$$\n\n* 순수하게 분류되면 0\n\n(3) 정보 이득\n\n* 엔트로피와 지니지수는 단지 속성의 불순도를 표현한다.\n\n* 우리가 알고 싶은 것은 **\"어떠한 속성이 얼마나 많은 정보를 제공하는가!\"** 이다.\n\n$$\\text {Gain}(T,X)= \\text{Entropy}(T)-\\text{Entropy}(T,X)$$\n\n* 위 식을 살펴보니 지니지수에서 했던 불순도 감소량과 비슷하지 않은가?\n\n $$\\Delta Gini(A) = Gini(D) - Gini_{A}(D)$$\n\n### (5) Ensemble\n\n* 앞서 언급한 tree는 과대적합의 위험이 큰 모형임 $\\to$ `max_depth`를 무작정 깊게 하면 과대적합이 발생하므로\n\n* 앙상블의 아이디어 : 이러한 test데이터 셋에 예측력이 약한 모델을 결합해서 성능이 좋은 모델을 만들자!\n\n#### Voting\n\n`-` 여러 모델들의 예측결과를 투표를 통해 최종 예측결과를 결정\n\n* 하드 보팅 : 다수 모델이 예측한 값이 최종 결과값\n\n* 소프트 보팅 : 모든 모델이 예측한 레이블 값의 **결정 확률 평균을 구한 뒤 가장 확률이 높은 값을 최종 선택**\n\n#### Bagging\n\n`-` Boostrap Aggregating\n\n`-` 아이디어 : 모형의 분산을 줄여 과적합을 방지하자.\n\n* 만약, 모집단으로부터 여러개의 훈련자료를 얻을 수 있고 이로부터 여러개의 모형 $\\hat{f}_1(x)\\dots \\hat{f}_b(x)$를 얻을 수 있다면, 다음과 같이 분산을 줄일 수 있다.\n\n$$\\hat{f}_{avg}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}_b(x)$$\n\n* 보통은 한 set의 자료만이 주어지게 되므로 위 방식은 직접 적용이 불가능\n\n* 그래서 우리는 복원추출을 기반으로  같은 size의 표본을 추출해 각각의 모델링을 수행한다. **(Bootstrap sample)**\n\n$$X_1^{*}\\dots X_B^{*}$$\n\n$$\\hat{f}_{\\text{bag}}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}^{*}_b(x)$$\n\n* 보팅과 다른점은 보팅은 여러개의 예측모델, 배깅은 동일한 예측모델 여러개를 앙상블하는 것임!\n\n* 대표적인 모델 : Randoms Forest\n\n##### random Forest\n\n`-` 여러 tree모델이 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링\n\n* 모델들이 개별적으로 학습을 수행한 뒤 모든 결과를 집계하여 최종 결과를 결정\n\n```python\n# 불러오기\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n```\n\n`-` Out-of-Bag은 생략 (ISLP 교재참고)\n\n`-` RF 모델의 변수 선택\n\n* 하나의 트리를 형성하는 과정에서, 각 노드에서 전체 $p$개의 설명변수 중 $m$개만을 임의로 추출하여 분리 규칙을 생성한다.\n\n    * 일반적으로 $m \\approx \\sqrt {p}$\n\n``` python\nRandomForestClassifier( max_features='sqrt') ## default\n```\n\n`-` 변수 중요도 (Variable Importance measur)\n\n* 사실 여러 형태의 나무를 결합하여 산출된 모델은.... 해석이 거의 불가능해진다.\n\n* 대안적으로, 나무들을 생성할 떄 어떠한 변수들이 `RSS` 혹은  `Gini index` 등에 큰 감소를 가져왔는지를 요약한 값으로 변수의 중요도를 파악할 수 있음.\n\n* $B$개의 모형에 대한 평균적인 기여 정도로 변수의 중요도를 평가하게 된다!\n\n*  [scikit-learn 참고링크](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n\n<center> <img src = \"변수중요도.png\" width = 500> </center>\n\n### (6) Boosting\n\n* 같은 유형의 약한 예측 모형을 결합하여 매우 정확한 예측 모형을 만드는 방법\n\n* 예측 모형을 순차적으로(Sequentially) 학습하여 먼저 학습된 모형의 결과가 다음 모형의 학습 시 정보를 제공\n\n* 즉, 이전 모형의 약점(잔차)를 학습하여 보완한다.\n\n<center> <img src = \"부스팅.png\" width = 500> </center>\n\n*  배깅에 비해 성능이 좋지만,  속도가 느리고 과적합 발생 가능성이 있음.\n\n* 대표적인 부스팅 알고리즘 : XGBoost, LightGBM\n\n`-` Boosting의 원리 (ISLP 기준)\n\n1. 초기값 셋팅 $\\hat{f}(x) = 0, r_1 = y_1$\n\n2.  For  $b = 1, 2\\dots B , repaet$ :\n\n$$\\hat {f}(x)_{i+1} = \\hat {f}(x)_{i} + \\lambda \\hat {f}^{b}(x)$$\n\n3. update the residual,\n\n\n$$r_{i+1} = r_{i}- \\lambda \\hat{f}^{b} (x) $$\n\n4. 초기 셋팅된 $\\hat {f}_1 = 0$ 이므로\n\n$$\\hat {f}(x)_{\\text{final}} = \\sum_{i=1}^{B} \\lambda \\hat {f}^{b}(x)$$\n\n`-` 위 같은 방식의 문제점  $\\to$ 과적합발생... 당연하다. 예측 모형을 순차적으로 학습한다는 것은 모형간 자기 상관성이 존재하고 모형의 분산이 증가하기 때문에 과적합이 발생할 수 밖에 없다...\n\n`-` 이를 막기위해 나온 모델이 XGBoost!\n\n#### XGBoost\n\n`-` [Extreme Gradient Boost](https://zephyrus1111.tistory.com/232)\n\n* review :  방금 정리했던 `Boosting`기법과 같이 기본 학습기를 의사결정나무로 하며, 잔차를 이용해 이전 모형의 약점을 보완하는 방식으로 학습한다.\n\n* +$\\alpha$ : 기존의 Graident Tree Boosting에 과적합 방지를 위한 파라미터$(\\lambda, \\gamma)$가 추가된 알고리즘이다.\n\n##### 알고리즘\n\n`0` parameter \n\n* $L$  : 손실함수\n\n* $M$ : 개별 모형의 최대 개수\n\n* $l$ : 학습률\n\n* $\\gamma, \\lambda$ : 과적합 방지를 위한 파라미터\n\n `1`. 초기 모형은 상수로 설정하며 다음과 같이 손실함수를 최소화 하는 모형으로 설정한다.\n \n - 초기 모형(상수값)을 아무렇게나 설정해도 된다고 하지만... 최적화 관점에서 아래처럼 잡아주는게 적절한 듯 하다.\n\n$$F_{0}(x) = \\underset {c}{\\text{arg min}}  \\sum_{i=1}^{n} L(y_i,c)  $$\n\n`2` $m = 1,\\dots M$에 대하여 다음을 반복\n\n* (a) Gradient $g_i$와 Hessian $h_i$를 계산\n\n$$g_i = \\left[ \\frac {\\partial L(y_i, F(x_i))}{\\partial  F(x_i)}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)$$\n\n$$h_i = \\left[ \\frac {\\partial^2 L(y_i, F(x_i))}{\\partial  F(x_i)^2}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)$$\n\n* (b) 회귀나무  $\\phi_m$을 다음과 같이 적합\n\n$$l = \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2$$\n\n$$\\phi_m = \\underset {\\phi} {\\text{arg min}} \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2$$\n\n* 여기서 $T$는 $\\phi$의 끝마디 개수, $||\\phi||^2 = \\sum_{j=1}^{T} w_j^2$ 이며 $w_j$는 $j$번째 끝마디에서의 출력값이다.\n\n    * 잘 살펴보면 릿지회귀분석에  L2 penalty와 비슷한데, L1 penalty 방식도 지원하는 것 같다.\n\n* (c) 다음과 같이 업데이트 한다.\n\n$$ F_{m}(x) = F_{m-1}(x) + l\\cdot \\phi_m(x)$$\n\n`3` 최종모형은?\n\n$$F_M(x) =  \\sum_{m=0}^{M} F_m(x)$$\n\n`4` summary\n\n* XGBoost는 기존 Gradient Boosting기법에 문제인 과적합문제를 해결하기 위해 $\\gamma, \\lambda$ 파라미터를 사용한다.(규제)\n\n* 손실함수를 살펴보면 터미널 노드(끝마디 노드)의 수와 끝마디에서의 출력값에 대한 패널티 파라미터가 들어가있다,\n\n* 의사결정나무에 가지치기 과정에서 터미널 노드의 개수에 따라 panelty를 부여하는 방식을 생각해보면 비슷한 방식이다.\n\n* 또한, 내장된 교차 검증? (이거는 이론적으로 구현되어있다기보단 사이킷런에서 내부적으로 동작하게 만든것 같음)\n\n    * 여튼 여기서 조기 중단을 가능하게끔 지원해준다.\n\n* 결측치 자체 처리 : 알아서 결측치를 고려해서 학습을 한다.(결측치 여부를 노드 분기를 위한 질문에 포함시킴)\n\n    * 이것도 사이킷런에서 내부적으로 구현한듯\n    \n    * 그래도 명시적으로 결측치에 대한 처리를 진행하기를 권고...\n\n`5` 실습 코드\n\n```python\n# 불러오기\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# 선언하기\nmodel = XGBRegressor(max_depth=5, n_estimators=100, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(mean_absolute_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))\n```\n\n`6` 주요 파라미터\n\n* learning_rate : 학습률(default = 0.1)\n\n* n_estimators : 나무의 개수 (default = 100)\n\n* mid_child_weight : 트리에서 추가적으로 분할할 지를 결정하기 위해 필요한 데이터들의 weight($w_i$)들의 총함 (default = 1)\n\n* gamma : 트리에서 추가적으로 분할할지를 결정하기 위한 값 $\\gamma T$ (default = 0)\n\n* max_depth : 나무의 깊이 (default = 6)\n\n* sub_sample : weak learner가 학습에 사용되는 데이터 샘플링 비율\n    * 과적합이 염려되는 경우 1보다 작은 값으로 설정, (default = 1)\n\n* colsample_bytree : 트리 생성에 필요한 변수 선택 비율\n    * 변수가 많은 경우 과적합을 조절하기 위해서 사용, 기본값 = 1\n \n* reg_lambda :  L2규제 적용값 ($\\lambda \\sum_{j=1}^{T} w_j^2$, 기본값 $\\lambda =  1$)\n\n* reg_alpha : L1규제 적용값 ($\\alpha \\sum_{j=1}^{T} |w|$, 기본값 $\\alpha = 0$)\n\n* early_stopping_rounds :  `n_estimators`만큼 학습을 반복하지 않더라도 조기 종료 가능(default = 10, 10번 동안 성능 향상이 없으면 학습 중단.)\n\n***\n\n### (7) SVM (분류)\n\n`-` 기본 아이디어 : 두 클래스 사이에 가장 넓은 도로를 내는 것\n\n`-` 용어 정리\n\n*  결정 경계 (Decision Boundary) or 초평면\n    * 클래스를 구분하는 경계선\n    * 결정 경계가 바로 모델 (Hyper plane)이라고 부름\n\n* 벡터 : 모든 데이터 포인트\n\n*  서포트 벡터 : 결정경계와 가끼운 데이터 포인트\n    * 마진의 크기와 결정경계에 영향을 미침\n\n* 마진(margin) : 서포트 벡터와 결정경계 사이의 거리\n\n    * 폭이 가장 넓은 도로를 찾는 것이 SVM의 목표\n    \n    * 마진이 클수록 새로운 데이터에 대해 안정적인 분류가 가능해지는 것임\n\n*  잘 생각해보면 마진의 크기가 좁을수록 정확한 분류가 일어나나 과적합 문제가 발생하므로 \n\n* **마진의 크기**와 **오류에 대한 허용 정도**는 Trade-off 관계에 있는 것을 알 수 있다.\n\n* 이것을 조절하는 파라미터 $\\to$ 비용(C)\n\n`-` 이를 이해하기 위해서!\n\n> Support vector classifier의 decision boundary는 다음과 같은 최적화 문제의 해로 정의된다.\n\n* 만약, label = {1,-1}이라 하면 초평면은 다음과 같은 성질을 가진다.\n\n$$\\text {Hyper plane}=  \\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} = 0$$\n\n$$\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} > 0, \\quad \\text{if }  y_i=1$$\n\n$$\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} < 0, \\quad \\text{if }  y_i=-1$$\n\n* 이는 다음과 같이 간단히 표현할 수 있다.\n\n$$y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) > 0$$\n\n* 아래의 두 식은 관측치가 초평면을 중심으로 두 class를 정확히 구분되고, 관측치와 초평면사이의 직교거리가 최소 $M$이상이 되도록 보장해 주는 조건이다.\n\n$$\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M} $$\n\n$$ \\sum_{i=1}^{p} \\beta_j^2 =  1$$\n\n$$y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) > M$$\n\n* 그러나  실제로 관측치는 두 개의 class로 정확히 구분되지 않은 경우가 더 많으며 한 두개의 관측치에 큰 영향을 받을 수 있다,(**Not robust**)\n\n> **관측치에 영향을 받아 초평면이 영향을 받은 예시** \n\n<center><img src = \"초평면.png\"></center>\n\n* 이를 해결하기 위해 $C>0$, tuning parameter와 $\\epsilon_i$(slack bariable)를 사용\n\n$$\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M}, \\sum_{i=1}^{p} \\beta_j^2 =  1 $$\n\n$$y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) > M(1-\\epsilon_i) \\quad  (\\epsilon_i > 0,  \\sum_{i=1}^n \\epsilon_i \\leq C)$$\n\n* $C$가 커질수록 margin이 넓어짐\n\n* $C$를 작게하면 현 데이터의 정확한 분류에 더 집중하게 되므로 자료 적합성이 좋아짐, 즉 bias는 감소하고 variance는 증가\n\n* margin 위 혹은 안쪽에 위치한 관측치들을 일컬어 **support vector**라고 한다.\n\n> C값에 따른 margin 변화\n\n<center> <img src = \"C.png\"  width = 400> </center>\n\n* 여기에서 $C$값이 가장 큰 것은 왼쪽 맨위 그림이다. $\\to$ 직관적으로 $C$는 허용한계이므로 $M$즉, 마진의 넓이를 넓힌다고 생각하자.\n\n* 잠깐 생각해야될 문제\n\n<center> <img src = \"nonlinear.png\"  width = 500> </center>\n\n* 우리는 여태껏 직선의 경우인 `Support vector classifier`를 생각했다. 근데 위에 처럼 생긴다면....\n\n`-` 그래서 고안된 방법이 본격적인 SVM(support vector machine)이다.\n\n* 고차원의 decision boundary를 고려함. $\\to$ 예를 들어, 2차항까지 고려한 최적화 문제의 해로써 decision boundary를 정의할 수 있음...\n\n$$\\underset{\\beta_0,\\beta_{11},\\dots,\\beta_{p1},\\beta_{12},\\dots,\\beta_{p2},\\varepsilon_1,\\dots ,\\varepsilon_n,M} {\\text {maximize}}M$$\n\n$$\\sum_{i=1}^{p}\\sum_{k=1}^2 \\beta_{jk}^2 = 1$$\n\n$$y_i \\left( \\beta_0 + \\sum_{j=1}^{p} \\beta_{j1}x_{ij} + \\sum_{j=1}^{p}\\beta_{i2}x_{ij}^2\\right) > M(1-\\varepsilon_i)$$\n\n$$\\epsilon_i > 0,  \\sum_{i=1}^n \\epsilon_i \\leq C$$\n\n`-` 단순히 확장한 것에 불과하지 않은가???\n\n* support vector classifier을 찾기 위해서는 관측치들간의 내적(inner product)을 계산하는 것으로 충분함이 알려져 있음\n\n* 이러한 내적을 여러 방면으로 일반화하여 표현할 수 있는데 이를 규정해 주는 함수를 `kernel` 이라 한다. (이 부분은 설명 생략!)\n\n`-` kernel 종류\n\n* poly(다항), rbf(Radial Basis Function), sigmoid, linear\n\n#### C값에 따른 margin 변화\n\n`-` 걍 넘어가려고 했는데 안되겠음 $\\to$ ㅅㅂ... 결과를 납득할 수 없다. 진짜 다시보자\n\n## 02. K-fold Cross Validation\n\n`-` 우리가 기존의 하던 방식\n\n```python\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 2023)\n\n```\n\n`-` 단점\n\n1.  랜덤하게 자료를 분할하기 때문에 분할결과에 따라 추정의 변동성이 크다....\n\n2. 특히 자료위 크기가 작거나 이상/영향치들이 포함되어 있는 경우에 더더욱 그러함.\n\n3. 또한, 원 자료의 크기 보다 작은 집합의 훈련자료가 모형적합에 사용되기 때문에 test error가 과대추정될 수 있음.\n\n* 이러한 방법을 $\\text {Validation set Approach}$라고 한다....\n\n`-` K-fold Cross Validation 방식\n\n<center><img src = \"kfold.png\" width = 500></center>\n\n* 전체 자료를 $k$개의 집합으로 분할한 후그 중 하나의 집합 ($i$)번째 집합을 평가자료롤 설정(위 그림의 경우 $i=1,2 \\dots 5$)\n\n* 그 후 각 정확도를 평균냄 (교제에서는 $MSE$를 평균 냈는데, 이번 강의에서는 정확도를 평균내더라...)\n\n$$\\text {CV}_{5} = \\frac {1}{5} \\sum_{i=1}^{5} \\text{Accuracy}_{i}$$\n\n```python\n# 1단계: 불러오기\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# 2단계: 선언하기\nmodel = DecisionTreeClassifier(max_depth=3)\n\n# 3단계: 검증하기\ncv_score = cross_val_score(model, x_train, y_train, cv=10)\n\n# 확인\nprint(cv_score)\nprint(cv_score.mean())\n\n```\n\n`=` 장점\n\n* 모든 데이터가 학습과 평가에 사용됨\n\n*  데이터가 부족해서 발생하는 과소적합 문제을 방지할 수 있음\n\n*  <font color = \"red\"> 좀 더 일반화된 모델</font> 을 만들 수 있음\n\n* test error의 과대추정을 방지\n\n`-`  단점\n\n* 반복 횟수가 많아서 모델 학습과 평가에 많은 시간이 소요\n\n***\n\n## 03. Search\n\n### (1) Grid search\n\n`-` 일단, 튜닝 시 모델들의 각 파라미터들에 값을 어떻게 하느냐에 따라 성능이 달라지는 것을 확인할 수 있었음\n\n`-` grid search의 아이디어는 가능한 파라미터 값 범위를 지정해 해당 범위에서의 값을 모두 사용하는 것이다.\n\n* 당연히 정확도는 높으나....시간이 오래 걸리겠지라는 생각을 해볼 수 있다.\n\n> 요런느낌\n``` python\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10), 'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n\n# Grid Search 선언\nmodel = GridSearchCV(knn_model, param, cv=3)\n\n```\n\n### (2) random search\n\n`-` 그리드 서치처럼 파라미터 범위를 지정하는 것은 동일\n\n`-` 설정한 파라미터 값 범위에서 몇 개를 선택할지를 정하여 `Random Search` 모델 선언 후 학습\n\n`-` 학습 데이터에 대해 가장 좋은 성능을 보인 파라미터 값으로 자동 학습함.\n\n* 참고로 Grid Search, Random Search를 사용할 때 내부적으로 **K-Fold Cross Validation**을 위해 cv값을 지정하므로!\n\n* 실제 수행되는 횟수는 <font color =\"red\"> 파라미터 조합 수 x CV값</font>이 된다.\n\n> code\n\n\n```python\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10),\n'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n# Random Search 선언\nmodel = RandomizedSearchCV(knn_model, \n                                                       param, cv=3, # (default = 5)\n                                                           n_iter=20) ### 전체 파라미터 범위에서 몇 개를 뽑을 것인지 (default = 10)\n```\n\n* 또한, 두 가지 기법을 섞어서 사용할 수 있음! (강의자료 참고)\n\n***\n\n## 04. 클래스 불균형\n\n`-` 머신러닝 알고리즘은 데이터가 클래스 간에 균형 있게 분포되어 있다고 가정함\n\n> example : 생존자와 사망자 수가 거의 같을 것이다~~\n\n`-` 클래스 불균형으로 인한 재현율이 형편없어지는 경우는 아래 링크를 참고!\n\n* [재현율](https://gangcheol.github.io/ISLP2023/posts/study/2023-09-03-00.%20%EB%8B%A8%EC%88%9C%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html#extra-2.-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C)\n\n#### 해결방법 (1) Resampling\n\n`-` under sampling\n\n* 다수 클래스 데이터를 소수 클래스 수 만큼 랜덤 샘플링\n\n`-` over sampling\n\n* 소수의 클래스 데이터를 다수 클래스 수 만큼 랜덤 샘플링\n\n```python\n# pip install imbalanced-learn\n# 불러오기\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Under Sampling\nunder_sample = RandomUnderSampler()\nu_x_train, u_y_train = under_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(u_y_train))\n\n# 불러오기\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Over Sampling\nover_sample = RandomOverSampler()\no_x_train, o_y_train = over_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(o_y_train))\n```\n\n`-` over sampling (2)\n\n```python\n# 불러오기\nfrom imblearn.over_sampling import SMOTE\n\n# Over Sampling\nsmote = SMOTE()\ns_x_train, s_y_train = smote.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(s_y_train))\n```\n\n#### 해결방법 (2) Class weight\n\n`-` resampling 없이 클래스에 가중치를 부여하여 클래스 불균형 문제를 해결해줌\n\n* 학습하는 동안 알고리즘의 비용함수에서 소수 클래스에 더 많은 가중치를 부여하여 소수 클래스에 더 높은 패널티를 제공함으로써, 소수 클래스에 대한 오류를 줄이게 됨\n\n* sklearn에서 제공하는 알고리즘 대부분 `class_weight`라는 하이퍼 파라미터를 제공한다.\n\n    * class_weight = 'None' : 기본값\n\n    * class_weight = 'balanced': y_train의 class 비율을 역으로 적용\n\n    * class_weight={0:0.2, 1:0.8}: 비율 지정, 단 비율의 합은 1\n\n\n* 주의! $\\to$ 전반적인 성능을 높이기 위한 작업이 아니라, 소수 클래스 성능을 높이기 위한 작업임!\n\n***\n\n##  06. excersise\n\n### 0. import\n\n### 1. 데이터 이해 및 준비\n\n`-` 쓸모없는 변수 제거\n\n`-` 결측치 확인\n\n`-` 현재 `arrival_delay_in_minutes`의 6개 결측치가 확인된다.\n\n* 결측치 제거를 위해 해당 변수의 분포를 확인하자.\n\n`-` 대부분의 값들이 0값 근처에 몰려있으니 결측값을 0으로 대체하자.\n\n`-` x, y 분리\n\n`-` 가변수화\n\n`-` 학습, 평가용 데이터 분리\n\n### 2. cv를 통한 최적의 모델 선택\n\n`-`  cv를 기준으로 XGB 모델이 최적의 모델인 것 같다.\n\n* grid search기법을 이용하여 모델 튜닝\n","srcMarkdownNoYaml":"\n\n# Supervised learning\n\n## 00. 모델링 단계\n\n### step1. 데이터 이해 및 준비\n\n`-` knn의 경우 필요하다면 스케일링 단계가 필요\n\n`-` 이산형 변수, 즉 범주형 변수를 모델의 예측변수로 사용할 경우 더미변수로 변환해주어야한다.\n\n```python\npd.get_dummies(data, columns = 더미화할컬럼리스트, dtype = (int or float))\n```\n\n`-` 결측치 처리 : 히스토그램, boxplot, 시계열 데이터인 경우 등등을 고려하여 각 case에 맞게 적절히 결측치를 처리해준다.\n\n* misforest, EM 알고리즘을 통한 결측치 처리를 한다지만, 개인적인 생각으로는 좀 과한 결측치 처리가 아닌지 싶음\n\n* 이유는 즉슨, 결측치를 처리하기위해 결측치 처리 단계에서 모델링을 한번 더 수행하는데 이 때 시간이 생각보다 오래 걸림\n\n### step2. cross-validation을 통한 최적의 모델 선택\n\n`-` 아래와 같이 여러개의 모델을 생성한다음 `cross-validation` 통해 최적의 모델을 선택하였다.\n\n> example \n\n\n```python\n\n# 1. knn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.cv = cross_val_score(knn, x_train_s, y_train, cv = 5)\n\nknn.cv_m = knn.cv.mean()\n\n# 2. tree\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth = 5, random_state = 1)\n\ntree.cv = cross_val_score(tree, x_train, y_train, cv = 5)\n\ntree.cv_m = tree.cv.mean()\n\n# 3. logistic\nfrom sklearn.linear_model import LogisticRegression\nlogit = LogisticRegression()\nlogit.cv = cross_val_score(logit, x_train, y_train, cv = 5)\n\nlogit.cv_m = logit.cv.mean()\n\n# 4. RF\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 5,  random_state = 1)\n\nrf.cv = cross_val_score(rf, x_train,y_train)\n\nrf.cv_m = rf.cv.mean()\n\n# 5. XGBoost\nfrom xgboost import XGBClassifier\nxgb =  XGBClassifier(max_depth = 5, random_state = 1)\n\nxgb.cv = cross_val_score(xgb, x_train, y_train, cv = 5)\n\nxgb.cv_m  = xgb.cv.mean()\n\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(max_depth = 5, random_state = 1,verbose = -100) \n\nlgbm.cv = cross_val_score(lgbm, x_train, y_train, cv = 5)\n\nlgbm.cv_m = lgbm.cv.mean()\n\n```\n\n### step 3. 모델 튜닝\n\n`-` 그 후 선택한 최종 모델을 튜닝해 최종 모델을 select\n\n```python\nfrom sklearn.model_selection import GridSearchCV\nmodel = XGBClassifier(max_depth= 5, random_state = 1)\n\nparams=  {\"max_depth\" : range(1,21)}\n\nmodel = GridSearchCV(model,\n                     params,\n                     cv=5,\n                     scoring='r2')\n\n```\n\n***\n\n## 01. 각 모델 소개\n\n### (1) regression model\n\n`-` 아래 링크를 참조해서 까먹을때 마다 보자~~\n\n`-` [ISLP2023-00.Linear Regression](https://gangcheol.github.io/ISLP2023/posts/2023-09-09-00.%20Linear%20Regression.html)\n\n### (2) 판별분석, 베이즈 분류기\n\n`-` 이것두 아래 링크를 참조하자.\n\n`-` [ISLP2023-01. Classification](https://gangcheol.github.io/ISLP2023/posts/2023-09-09-01.%20classification.html#%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9Ddiscriminant-analysis)\n\n### (3) KNN (K-nearest neighbors)\n\n`-` 학습을 안함, 그냥 그 근처 K개의 녀석들을 보고 값을 할당\n\n$$P(y= j | X = x_0) = \\frac {1}{K}  \\sum_{i\\in N_0} I(y_i = j)$$\n\n$$N_0  : x_0\\text {와 가장 가까운  K개의 자료의 집합}$$\n\n`-` $k$가 작을수록 모델은 복잡해지고, 클수록 단순해짐\n\n* 솔직히 와닿지 않지만, 내 방식대로 이해해보자.\n\n* 이전에 선형회귀분석에서 모델 복잡도를 생각해보자, 모델을 일직선으로 예측한 경우 단순선형회귀분석이다.\n\n* 즉, 모델 하나하나의 포인트를 고려하지 않고 전체 평균적인 선형회귀식을 하나 구한 것이다.\n\n* 이를 다시 KNN예제로 생각해 $K$가 클경우 생각해보변, 주변 녀석들의 하나하나 개인적은 특성을 고려하기보단 전체적인 특성에 기반하여 주어진$x_0$에 대한 $y$를 예측하는 것이다.\n\n* 따라서, $k$가 작을수록 주변 녀석들의 특징을 하나하나 잘 고려해서 모델이 복잡한 것이고, $k$가 크면 전체적인 평균을 고려한 것이기 때문에 모델이 단순해진다...\n$\\to$ 사실 이것도 그렇게 와닿지 않음 나중에 더 찾아보자...\n\n### (4) Decsion Tree\n\n>  나무모형은 간단하고 해석상에 장점이 있으나 다른 방법들에 비해 좋은 성능을 보이지 못하는 경우가 있음\n\n#### 형성단계\n\n`1`  설명변수들의 가능한 조합을 이용하여 예측공간을 $J$개의 겹치지 않는(non-overlaping)구역으로 분할\n\n`2` 각 관측값은 $R_j$ 구역에 포함되며, $R_j$ 구역에 포함된 training data의 반응변수 ($y$)의 평균  (분류문제에선 voting방식)을 이용하여 예측\n\n$$\\hat {y}_{R_j} = \\frac {1}{n_j} \\sum_{k\\in R_J} y_k $$\n\n`3` 목표 : 다음의 RSS를 최소화 하는 구역 $R_1,R_2, R_J$를 찾는 것\n\n$$RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat {y}_{R_{j}})^2$$\n\n`4` 모든 조합을 확인하는 것은 불가능...(사실 가능하다. tree를 무한정 쪼개면)\n\n* 근데  tree를 무한정 쪼갤경우 과적합문제가 무조건 발생\n\n`5`  정지규칙 (stopping rule)\n\n* 모든 자료가 한 범주에 속할 때\n\n* 노드에 속하는 자료가 일정 수 이하일 떄 \n\n* MSE의 감소량이 아주 작을 떄 \n\n* 뿌리마디로부터의 깊이가 일정 수 이상일 떄 등 (max_depth)\n\n`6` 가지치기 : 과적합을 막기위한 방법\n\n* 사실 정지규칙도 이에 포함됨, 따라서 위에거 + 빠진 내용을 적겠음\n\n*  **min_samples_leaf(default = 1)** : leaf노드가 되기 위한 최소한의 샘플 수\n\n*  **min_samples_split(default = 20** : 노드를 분할하기 위한 최소한의 샘플 수 (값을 적게 설정할수록 계순 분할되어, 과적합 발생 위험 증가)\n\n* **max_feature** : 최선의 분할을 위해 고려할 변수(feature) 개수\n\n    * sqrt : 전체 변수 개수의 루트\n    \n    * auto :  sqrt와 같은 의미\n    \n    * log : $\\log_{2}$ (전체 변수의 수)\n\n*  max_leaf_node : 리프 노드의 최대 개수 $\\to$ $\\text{Cost complexity Pruning}$\n\n     * $|T|$는 터미널도드로 리프노드의 개수를 뜻한다.\n     \n     *  $R_{\\alpha}(T)$는 변하지 않는 비용함수로 $R(T)$는 우리가 알고 있는 $RSS$와 같다.\n     \n     * 아래식이 뜻하는 바는  리프노드의 개수가 클수록  $R(T)$ 훈련 데이터 셋에대한 $RSS$가 작아져 과적합 문제가 발생할 수 있기 때문에 적절한 리프노드의 개수를 설정해야한다는 의미이다.\n     \n     * $\\alpha$는 $\\text {tuning parameter}$로 복잡도를 조절한다. 만약 $\\alpha$가 0이라면 기존의 비용함수와 같고 1에 가까워질수록 $R(T)$값이 작아진다.\n     \n     * 따라서 우리는 적절한 $\\alpha$값과 $|T|$값을 교차검증 기법을 통해 찾아내어 가지치기를 수행하여야한다.\n\n$$\\begin {align}R_{\\alpha}(T) &= \\sum R(T) + \\alpha |T|  \\\\ \\\\\n                                                          &= \\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat {y}_{R_m})^2 +  \\alpha |T|  \\end {align}$$\n\n`7` 비용함수\n\n(1) 지니 지수 (Gini Index)\n\n$$Gini (D) = 1- \\sum_{k=1}^{K}p_{k}^2 = \\sum_{k=1}^{K} p_k(1-p_k)$$\n\n$$p_k : \\text{Node D에서 k번째 범주에 속하는 관측 비율}$$\n\n<center><img src = \"GINI.png\" width= 500> </center>\n\n* 순수하게 분류되면 값은 0이다.\n\n* 만약 분리규칙 $A$에 의해서 **Node D**가 $D_1, D_2$로 분리된다면, 분리규칙 $A$에서 Ginin지수는 다음과 같다.\n\n$$Gini_A(D) =\\frac {|D_1|}{|D|}Gini(D_1) +\\frac {|D_2|}{|D|}Gini(D_2) $$\n\n* 위에 근거하여 분리규칙 A에서 발생한 **불순도 감소량**은 다음과 같이 정의할 수 있다.\n\n$$\\Delta Gini(A) = Gini(D) - Gini_{A}(D)$$\n\n* 따라서, $Gini_{A}(D)$를 가장 작게 하거나 $\\Delta Gini(A)$를 가장 크게 하는 분리 규칙을 선택!\n\n***\n\n(2)  엔트로피(Entropy)\n\n$$\\text {Entropy}  = -\\sum_{i=1}^m p_i\\log_{2} p_i$$\n\n* 순수하게 분류되면 0\n\n(3) 정보 이득\n\n* 엔트로피와 지니지수는 단지 속성의 불순도를 표현한다.\n\n* 우리가 알고 싶은 것은 **\"어떠한 속성이 얼마나 많은 정보를 제공하는가!\"** 이다.\n\n$$\\text {Gain}(T,X)= \\text{Entropy}(T)-\\text{Entropy}(T,X)$$\n\n* 위 식을 살펴보니 지니지수에서 했던 불순도 감소량과 비슷하지 않은가?\n\n $$\\Delta Gini(A) = Gini(D) - Gini_{A}(D)$$\n\n### (5) Ensemble\n\n* 앞서 언급한 tree는 과대적합의 위험이 큰 모형임 $\\to$ `max_depth`를 무작정 깊게 하면 과대적합이 발생하므로\n\n* 앙상블의 아이디어 : 이러한 test데이터 셋에 예측력이 약한 모델을 결합해서 성능이 좋은 모델을 만들자!\n\n#### Voting\n\n`-` 여러 모델들의 예측결과를 투표를 통해 최종 예측결과를 결정\n\n* 하드 보팅 : 다수 모델이 예측한 값이 최종 결과값\n\n* 소프트 보팅 : 모든 모델이 예측한 레이블 값의 **결정 확률 평균을 구한 뒤 가장 확률이 높은 값을 최종 선택**\n\n#### Bagging\n\n`-` Boostrap Aggregating\n\n`-` 아이디어 : 모형의 분산을 줄여 과적합을 방지하자.\n\n* 만약, 모집단으로부터 여러개의 훈련자료를 얻을 수 있고 이로부터 여러개의 모형 $\\hat{f}_1(x)\\dots \\hat{f}_b(x)$를 얻을 수 있다면, 다음과 같이 분산을 줄일 수 있다.\n\n$$\\hat{f}_{avg}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}_b(x)$$\n\n* 보통은 한 set의 자료만이 주어지게 되므로 위 방식은 직접 적용이 불가능\n\n* 그래서 우리는 복원추출을 기반으로  같은 size의 표본을 추출해 각각의 모델링을 수행한다. **(Bootstrap sample)**\n\n$$X_1^{*}\\dots X_B^{*}$$\n\n$$\\hat{f}_{\\text{bag}}(x) = \\frac {1}{B} \\sum_{i=1}^{B} \\hat{f}^{*}_b(x)$$\n\n* 보팅과 다른점은 보팅은 여러개의 예측모델, 배깅은 동일한 예측모델 여러개를 앙상블하는 것임!\n\n* 대표적인 모델 : Randoms Forest\n\n##### random Forest\n\n`-` 여러 tree모델이 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링\n\n* 모델들이 개별적으로 학습을 수행한 뒤 모든 결과를 집계하여 최종 결과를 결정\n\n```python\n# 불러오기\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import *\n\n# 선언하기\nmodel = RandomForestClassifier(max_depth=5, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n```\n\n`-` Out-of-Bag은 생략 (ISLP 교재참고)\n\n`-` RF 모델의 변수 선택\n\n* 하나의 트리를 형성하는 과정에서, 각 노드에서 전체 $p$개의 설명변수 중 $m$개만을 임의로 추출하여 분리 규칙을 생성한다.\n\n    * 일반적으로 $m \\approx \\sqrt {p}$\n\n``` python\nRandomForestClassifier( max_features='sqrt') ## default\n```\n\n`-` 변수 중요도 (Variable Importance measur)\n\n* 사실 여러 형태의 나무를 결합하여 산출된 모델은.... 해석이 거의 불가능해진다.\n\n* 대안적으로, 나무들을 생성할 떄 어떠한 변수들이 `RSS` 혹은  `Gini index` 등에 큰 감소를 가져왔는지를 요약한 값으로 변수의 중요도를 파악할 수 있음.\n\n* $B$개의 모형에 대한 평균적인 기여 정도로 변수의 중요도를 평가하게 된다!\n\n*  [scikit-learn 참고링크](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n\n<center> <img src = \"변수중요도.png\" width = 500> </center>\n\n### (6) Boosting\n\n* 같은 유형의 약한 예측 모형을 결합하여 매우 정확한 예측 모형을 만드는 방법\n\n* 예측 모형을 순차적으로(Sequentially) 학습하여 먼저 학습된 모형의 결과가 다음 모형의 학습 시 정보를 제공\n\n* 즉, 이전 모형의 약점(잔차)를 학습하여 보완한다.\n\n<center> <img src = \"부스팅.png\" width = 500> </center>\n\n*  배깅에 비해 성능이 좋지만,  속도가 느리고 과적합 발생 가능성이 있음.\n\n* 대표적인 부스팅 알고리즘 : XGBoost, LightGBM\n\n`-` Boosting의 원리 (ISLP 기준)\n\n1. 초기값 셋팅 $\\hat{f}(x) = 0, r_1 = y_1$\n\n2.  For  $b = 1, 2\\dots B , repaet$ :\n\n$$\\hat {f}(x)_{i+1} = \\hat {f}(x)_{i} + \\lambda \\hat {f}^{b}(x)$$\n\n3. update the residual,\n\n\n$$r_{i+1} = r_{i}- \\lambda \\hat{f}^{b} (x) $$\n\n4. 초기 셋팅된 $\\hat {f}_1 = 0$ 이므로\n\n$$\\hat {f}(x)_{\\text{final}} = \\sum_{i=1}^{B} \\lambda \\hat {f}^{b}(x)$$\n\n`-` 위 같은 방식의 문제점  $\\to$ 과적합발생... 당연하다. 예측 모형을 순차적으로 학습한다는 것은 모형간 자기 상관성이 존재하고 모형의 분산이 증가하기 때문에 과적합이 발생할 수 밖에 없다...\n\n`-` 이를 막기위해 나온 모델이 XGBoost!\n\n#### XGBoost\n\n`-` [Extreme Gradient Boost](https://zephyrus1111.tistory.com/232)\n\n* review :  방금 정리했던 `Boosting`기법과 같이 기본 학습기를 의사결정나무로 하며, 잔차를 이용해 이전 모형의 약점을 보완하는 방식으로 학습한다.\n\n* +$\\alpha$ : 기존의 Graident Tree Boosting에 과적합 방지를 위한 파라미터$(\\lambda, \\gamma)$가 추가된 알고리즘이다.\n\n##### 알고리즘\n\n`0` parameter \n\n* $L$  : 손실함수\n\n* $M$ : 개별 모형의 최대 개수\n\n* $l$ : 학습률\n\n* $\\gamma, \\lambda$ : 과적합 방지를 위한 파라미터\n\n `1`. 초기 모형은 상수로 설정하며 다음과 같이 손실함수를 최소화 하는 모형으로 설정한다.\n \n - 초기 모형(상수값)을 아무렇게나 설정해도 된다고 하지만... 최적화 관점에서 아래처럼 잡아주는게 적절한 듯 하다.\n\n$$F_{0}(x) = \\underset {c}{\\text{arg min}}  \\sum_{i=1}^{n} L(y_i,c)  $$\n\n`2` $m = 1,\\dots M$에 대하여 다음을 반복\n\n* (a) Gradient $g_i$와 Hessian $h_i$를 계산\n\n$$g_i = \\left[ \\frac {\\partial L(y_i, F(x_i))}{\\partial  F(x_i)}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)$$\n\n$$h_i = \\left[ \\frac {\\partial^2 L(y_i, F(x_i))}{\\partial  F(x_i)^2}\\right],\\quad F(x_i) = \\hat {F}_{m-1}(x)$$\n\n* (b) 회귀나무  $\\phi_m$을 다음과 같이 적합\n\n$$l = \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2$$\n\n$$\\phi_m = \\underset {\\phi} {\\text{arg min}} \\sum_{i=1}^{n} \\frac {1}{2}h_i \\left [ - \\frac {g_i}{h_i} - \\phi(x_i) \\right ]  + \\gamma T + \\frac {1}{2} \\lambda ||\\phi||^2$$\n\n* 여기서 $T$는 $\\phi$의 끝마디 개수, $||\\phi||^2 = \\sum_{j=1}^{T} w_j^2$ 이며 $w_j$는 $j$번째 끝마디에서의 출력값이다.\n\n    * 잘 살펴보면 릿지회귀분석에  L2 penalty와 비슷한데, L1 penalty 방식도 지원하는 것 같다.\n\n* (c) 다음과 같이 업데이트 한다.\n\n$$ F_{m}(x) = F_{m-1}(x) + l\\cdot \\phi_m(x)$$\n\n`3` 최종모형은?\n\n$$F_M(x) =  \\sum_{m=0}^{M} F_m(x)$$\n\n`4` summary\n\n* XGBoost는 기존 Gradient Boosting기법에 문제인 과적합문제를 해결하기 위해 $\\gamma, \\lambda$ 파라미터를 사용한다.(규제)\n\n* 손실함수를 살펴보면 터미널 노드(끝마디 노드)의 수와 끝마디에서의 출력값에 대한 패널티 파라미터가 들어가있다,\n\n* 의사결정나무에 가지치기 과정에서 터미널 노드의 개수에 따라 panelty를 부여하는 방식을 생각해보면 비슷한 방식이다.\n\n* 또한, 내장된 교차 검증? (이거는 이론적으로 구현되어있다기보단 사이킷런에서 내부적으로 동작하게 만든것 같음)\n\n    * 여튼 여기서 조기 중단을 가능하게끔 지원해준다.\n\n* 결측치 자체 처리 : 알아서 결측치를 고려해서 학습을 한다.(결측치 여부를 노드 분기를 위한 질문에 포함시킴)\n\n    * 이것도 사이킷런에서 내부적으로 구현한듯\n    \n    * 그래도 명시적으로 결측치에 대한 처리를 진행하기를 권고...\n\n`5` 실습 코드\n\n```python\n# 불러오기\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# 선언하기\nmodel = XGBRegressor(max_depth=5, n_estimators=100, random_state=1)\n\n# 학습하기\nmodel.fit(x_train, y_train)\n\n# 예측하기\ny_pred = model.predict(x_test)\n\n# 평가하기\nprint(mean_absolute_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))\n```\n\n`6` 주요 파라미터\n\n* learning_rate : 학습률(default = 0.1)\n\n* n_estimators : 나무의 개수 (default = 100)\n\n* mid_child_weight : 트리에서 추가적으로 분할할 지를 결정하기 위해 필요한 데이터들의 weight($w_i$)들의 총함 (default = 1)\n\n* gamma : 트리에서 추가적으로 분할할지를 결정하기 위한 값 $\\gamma T$ (default = 0)\n\n* max_depth : 나무의 깊이 (default = 6)\n\n* sub_sample : weak learner가 학습에 사용되는 데이터 샘플링 비율\n    * 과적합이 염려되는 경우 1보다 작은 값으로 설정, (default = 1)\n\n* colsample_bytree : 트리 생성에 필요한 변수 선택 비율\n    * 변수가 많은 경우 과적합을 조절하기 위해서 사용, 기본값 = 1\n \n* reg_lambda :  L2규제 적용값 ($\\lambda \\sum_{j=1}^{T} w_j^2$, 기본값 $\\lambda =  1$)\n\n* reg_alpha : L1규제 적용값 ($\\alpha \\sum_{j=1}^{T} |w|$, 기본값 $\\alpha = 0$)\n\n* early_stopping_rounds :  `n_estimators`만큼 학습을 반복하지 않더라도 조기 종료 가능(default = 10, 10번 동안 성능 향상이 없으면 학습 중단.)\n\n***\n\n### (7) SVM (분류)\n\n`-` 기본 아이디어 : 두 클래스 사이에 가장 넓은 도로를 내는 것\n\n`-` 용어 정리\n\n*  결정 경계 (Decision Boundary) or 초평면\n    * 클래스를 구분하는 경계선\n    * 결정 경계가 바로 모델 (Hyper plane)이라고 부름\n\n* 벡터 : 모든 데이터 포인트\n\n*  서포트 벡터 : 결정경계와 가끼운 데이터 포인트\n    * 마진의 크기와 결정경계에 영향을 미침\n\n* 마진(margin) : 서포트 벡터와 결정경계 사이의 거리\n\n    * 폭이 가장 넓은 도로를 찾는 것이 SVM의 목표\n    \n    * 마진이 클수록 새로운 데이터에 대해 안정적인 분류가 가능해지는 것임\n\n*  잘 생각해보면 마진의 크기가 좁을수록 정확한 분류가 일어나나 과적합 문제가 발생하므로 \n\n* **마진의 크기**와 **오류에 대한 허용 정도**는 Trade-off 관계에 있는 것을 알 수 있다.\n\n* 이것을 조절하는 파라미터 $\\to$ 비용(C)\n\n`-` 이를 이해하기 위해서!\n\n> Support vector classifier의 decision boundary는 다음과 같은 최적화 문제의 해로 정의된다.\n\n* 만약, label = {1,-1}이라 하면 초평면은 다음과 같은 성질을 가진다.\n\n$$\\text {Hyper plane}=  \\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} = 0$$\n\n$$\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} > 0, \\quad \\text{if }  y_i=1$$\n\n$$\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip} < 0, \\quad \\text{if }  y_i=-1$$\n\n* 이는 다음과 같이 간단히 표현할 수 있다.\n\n$$y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) > 0$$\n\n* 아래의 두 식은 관측치가 초평면을 중심으로 두 class를 정확히 구분되고, 관측치와 초평면사이의 직교거리가 최소 $M$이상이 되도록 보장해 주는 조건이다.\n\n$$\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M} $$\n\n$$ \\sum_{i=1}^{p} \\beta_j^2 =  1$$\n\n$$y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) > M$$\n\n* 그러나  실제로 관측치는 두 개의 class로 정확히 구분되지 않은 경우가 더 많으며 한 두개의 관측치에 큰 영향을 받을 수 있다,(**Not robust**)\n\n> **관측치에 영향을 받아 초평면이 영향을 받은 예시** \n\n<center><img src = \"초평면.png\"></center>\n\n* 이를 해결하기 위해 $C>0$, tuning parameter와 $\\epsilon_i$(slack bariable)를 사용\n\n$$\\underset{\\beta_0,\\beta_1 \\dots \\beta_p, M}{\\text {maximize} M}, \\sum_{i=1}^{p} \\beta_j^2 =  1 $$\n\n$$y_i(\\beta_0+ \\beta_1x_{i1}+\\dots \\beta_1x_{ip}) > M(1-\\epsilon_i) \\quad  (\\epsilon_i > 0,  \\sum_{i=1}^n \\epsilon_i \\leq C)$$\n\n* $C$가 커질수록 margin이 넓어짐\n\n* $C$를 작게하면 현 데이터의 정확한 분류에 더 집중하게 되므로 자료 적합성이 좋아짐, 즉 bias는 감소하고 variance는 증가\n\n* margin 위 혹은 안쪽에 위치한 관측치들을 일컬어 **support vector**라고 한다.\n\n> C값에 따른 margin 변화\n\n<center> <img src = \"C.png\"  width = 400> </center>\n\n* 여기에서 $C$값이 가장 큰 것은 왼쪽 맨위 그림이다. $\\to$ 직관적으로 $C$는 허용한계이므로 $M$즉, 마진의 넓이를 넓힌다고 생각하자.\n\n* 잠깐 생각해야될 문제\n\n<center> <img src = \"nonlinear.png\"  width = 500> </center>\n\n* 우리는 여태껏 직선의 경우인 `Support vector classifier`를 생각했다. 근데 위에 처럼 생긴다면....\n\n`-` 그래서 고안된 방법이 본격적인 SVM(support vector machine)이다.\n\n* 고차원의 decision boundary를 고려함. $\\to$ 예를 들어, 2차항까지 고려한 최적화 문제의 해로써 decision boundary를 정의할 수 있음...\n\n$$\\underset{\\beta_0,\\beta_{11},\\dots,\\beta_{p1},\\beta_{12},\\dots,\\beta_{p2},\\varepsilon_1,\\dots ,\\varepsilon_n,M} {\\text {maximize}}M$$\n\n$$\\sum_{i=1}^{p}\\sum_{k=1}^2 \\beta_{jk}^2 = 1$$\n\n$$y_i \\left( \\beta_0 + \\sum_{j=1}^{p} \\beta_{j1}x_{ij} + \\sum_{j=1}^{p}\\beta_{i2}x_{ij}^2\\right) > M(1-\\varepsilon_i)$$\n\n$$\\epsilon_i > 0,  \\sum_{i=1}^n \\epsilon_i \\leq C$$\n\n`-` 단순히 확장한 것에 불과하지 않은가???\n\n* support vector classifier을 찾기 위해서는 관측치들간의 내적(inner product)을 계산하는 것으로 충분함이 알려져 있음\n\n* 이러한 내적을 여러 방면으로 일반화하여 표현할 수 있는데 이를 규정해 주는 함수를 `kernel` 이라 한다. (이 부분은 설명 생략!)\n\n`-` kernel 종류\n\n* poly(다항), rbf(Radial Basis Function), sigmoid, linear\n\n#### C값에 따른 margin 변화\n\n`-` 걍 넘어가려고 했는데 안되겠음 $\\to$ ㅅㅂ... 결과를 납득할 수 없다. 진짜 다시보자\n\n## 02. K-fold Cross Validation\n\n`-` 우리가 기존의 하던 방식\n\n```python\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 2023)\n\n```\n\n`-` 단점\n\n1.  랜덤하게 자료를 분할하기 때문에 분할결과에 따라 추정의 변동성이 크다....\n\n2. 특히 자료위 크기가 작거나 이상/영향치들이 포함되어 있는 경우에 더더욱 그러함.\n\n3. 또한, 원 자료의 크기 보다 작은 집합의 훈련자료가 모형적합에 사용되기 때문에 test error가 과대추정될 수 있음.\n\n* 이러한 방법을 $\\text {Validation set Approach}$라고 한다....\n\n`-` K-fold Cross Validation 방식\n\n<center><img src = \"kfold.png\" width = 500></center>\n\n* 전체 자료를 $k$개의 집합으로 분할한 후그 중 하나의 집합 ($i$)번째 집합을 평가자료롤 설정(위 그림의 경우 $i=1,2 \\dots 5$)\n\n* 그 후 각 정확도를 평균냄 (교제에서는 $MSE$를 평균 냈는데, 이번 강의에서는 정확도를 평균내더라...)\n\n$$\\text {CV}_{5} = \\frac {1}{5} \\sum_{i=1}^{5} \\text{Accuracy}_{i}$$\n\n```python\n# 1단계: 불러오기\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# 2단계: 선언하기\nmodel = DecisionTreeClassifier(max_depth=3)\n\n# 3단계: 검증하기\ncv_score = cross_val_score(model, x_train, y_train, cv=10)\n\n# 확인\nprint(cv_score)\nprint(cv_score.mean())\n\n```\n\n`=` 장점\n\n* 모든 데이터가 학습과 평가에 사용됨\n\n*  데이터가 부족해서 발생하는 과소적합 문제을 방지할 수 있음\n\n*  <font color = \"red\"> 좀 더 일반화된 모델</font> 을 만들 수 있음\n\n* test error의 과대추정을 방지\n\n`-`  단점\n\n* 반복 횟수가 많아서 모델 학습과 평가에 많은 시간이 소요\n\n***\n\n## 03. Search\n\n### (1) Grid search\n\n`-` 일단, 튜닝 시 모델들의 각 파라미터들에 값을 어떻게 하느냐에 따라 성능이 달라지는 것을 확인할 수 있었음\n\n`-` grid search의 아이디어는 가능한 파라미터 값 범위를 지정해 해당 범위에서의 값을 모두 사용하는 것이다.\n\n* 당연히 정확도는 높으나....시간이 오래 걸리겠지라는 생각을 해볼 수 있다.\n\n> 요런느낌\n``` python\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10), 'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n\n# Grid Search 선언\nmodel = GridSearchCV(knn_model, param, cv=3)\n\n```\n\n### (2) random search\n\n`-` 그리드 서치처럼 파라미터 범위를 지정하는 것은 동일\n\n`-` 설정한 파라미터 값 범위에서 몇 개를 선택할지를 정하여 `Random Search` 모델 선언 후 학습\n\n`-` 학습 데이터에 대해 가장 좋은 성능을 보인 파라미터 값으로 자동 학습함.\n\n* 참고로 Grid Search, Random Search를 사용할 때 내부적으로 **K-Fold Cross Validation**을 위해 cv값을 지정하므로!\n\n* 실제 수행되는 횟수는 <font color =\"red\"> 파라미터 조합 수 x CV값</font>이 된다.\n\n> code\n\n\n```python\n# 함수 불러오기\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# 파라미터 선언\nparam = {'n_neighbors': range(1, 500, 10),\n'metric': ['euclidean', 'manhattan']}\n\n# 기본모델 선언\nknn_model = KNeighborsClassifier()\n# Random Search 선언\nmodel = RandomizedSearchCV(knn_model, \n                                                       param, cv=3, # (default = 5)\n                                                           n_iter=20) ### 전체 파라미터 범위에서 몇 개를 뽑을 것인지 (default = 10)\n```\n\n* 또한, 두 가지 기법을 섞어서 사용할 수 있음! (강의자료 참고)\n\n***\n\n## 04. 클래스 불균형\n\n`-` 머신러닝 알고리즘은 데이터가 클래스 간에 균형 있게 분포되어 있다고 가정함\n\n> example : 생존자와 사망자 수가 거의 같을 것이다~~\n\n`-` 클래스 불균형으로 인한 재현율이 형편없어지는 경우는 아래 링크를 참고!\n\n* [재현율](https://gangcheol.github.io/ISLP2023/posts/study/2023-09-03-00.%20%EB%8B%A8%EC%88%9C%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html#extra-2.-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C)\n\n#### 해결방법 (1) Resampling\n\n`-` under sampling\n\n* 다수 클래스 데이터를 소수 클래스 수 만큼 랜덤 샘플링\n\n`-` over sampling\n\n* 소수의 클래스 데이터를 다수 클래스 수 만큼 랜덤 샘플링\n\n```python\n# pip install imbalanced-learn\n# 불러오기\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Under Sampling\nunder_sample = RandomUnderSampler()\nu_x_train, u_y_train = under_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(u_y_train))\n\n# 불러오기\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Over Sampling\nover_sample = RandomOverSampler()\no_x_train, o_y_train = over_sample.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(o_y_train))\n```\n\n`-` over sampling (2)\n\n```python\n# 불러오기\nfrom imblearn.over_sampling import SMOTE\n\n# Over Sampling\nsmote = SMOTE()\ns_x_train, s_y_train = smote.fit_resample(x_train, y_train)\n\n# 확인\nprint('전:', np.bincount(y_train))\nprint('후:', np.bincount(s_y_train))\n```\n\n#### 해결방법 (2) Class weight\n\n`-` resampling 없이 클래스에 가중치를 부여하여 클래스 불균형 문제를 해결해줌\n\n* 학습하는 동안 알고리즘의 비용함수에서 소수 클래스에 더 많은 가중치를 부여하여 소수 클래스에 더 높은 패널티를 제공함으로써, 소수 클래스에 대한 오류를 줄이게 됨\n\n* sklearn에서 제공하는 알고리즘 대부분 `class_weight`라는 하이퍼 파라미터를 제공한다.\n\n    * class_weight = 'None' : 기본값\n\n    * class_weight = 'balanced': y_train의 class 비율을 역으로 적용\n\n    * class_weight={0:0.2, 1:0.8}: 비율 지정, 단 비율의 합은 1\n\n\n* 주의! $\\to$ 전반적인 성능을 높이기 위한 작업이 아니라, 소수 클래스 성능을 높이기 위한 작업임!\n\n***\n\n##  06. excersise\n\n### 0. import\n\n### 1. 데이터 이해 및 준비\n\n`-` 쓸모없는 변수 제거\n\n`-` 결측치 확인\n\n`-` 현재 `arrival_delay_in_minutes`의 6개 결측치가 확인된다.\n\n* 결측치 제거를 위해 해당 변수의 분포를 확인하자.\n\n`-` 대부분의 값들이 0값 근처에 몰려있으니 결측값을 0으로 대체하자.\n\n`-` x, y 분리\n\n`-` 가변수화\n\n`-` 학습, 평가용 데이터 분리\n\n### 2. cv를 통한 최적의 모델 선택\n\n`-`  cv를 기준으로 XGB 모델이 최적의 모델인 것 같다.\n\n* grid search기법을 이용하여 모델 튜닝\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-01-21-Extra 01. summary (1).html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","editor":"visual","code-copy":true,"title-block-banner":true,"title":"Extra 01. summary (1)","author":"GC","date":"01/21/23","categories":["python"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}